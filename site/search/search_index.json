{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"BenchBBoxTest","text":"<p>A Python package for benchmarking black-box testing methods.</p>"},{"location":"#overview","title":"Overview","text":"<p>BenchBBoxTest provides tools and utilities for generating synthetic datasets and evaluating black-box testing methodologies. The package includes modules for:</p> <ul> <li>Text Generation: Using language models to generate text datasets</li> <li>Image Generation: Creating image datasets with specific properties</li> <li>Simulation: Simulating data from various distributions</li> <li>Evaluation: Tools for evaluating testing methods</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install benchbboxtest\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>from benchbboxtest.datasets.text import LLMGenerator\n\n# Initialize a language model generator\ngenerator = LLMGenerator(model_name=\"gpt2\")\n\n# Generate text based on a prompt\ntext = generator.generate_text(\n    prompt=\"This is an example of text generation\",\n    max_length=100\n)\n\nprint(text)\n</code></pre>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details. </p>"},{"location":"contributing/","title":"Contributing to BenchBBoxTest","text":"<p>We welcome contributions to BenchBBoxTest! This document provides guidelines and instructions for contributing.</p>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>Please be respectful and considerate of others when contributing to the project.</p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":"<ol> <li>Fork the repository on GitHub</li> <li>Clone your fork locally</li> <li>Set up your development environment</li> <li>Create a new branch for your feature or bug fix</li> </ol>"},{"location":"contributing/#development-environment","title":"Development Environment","text":"<p>Set up your development environment:</p> <pre><code># Clone the repository\ngit clone https://github.com/jiantingfeng/BenchBBoxTest.git\ncd BenchBBoxTest\n\n# Create a virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install the package in development mode\npip install -e .\n\n# Install development dependencies\npip install -r requirements-dev.txt\n</code></pre>"},{"location":"contributing/#making-changes","title":"Making Changes","text":"<ol> <li> <p>Create a new branch for your changes:    <pre><code>git checkout -b feature/your-feature-name\n</code></pre></p> </li> <li> <p>Make your changes, following the coding style of the project</p> </li> <li> <p>Add tests for your changes</p> </li> <li> <p>Run the tests:    <pre><code>pytest\n</code></pre></p> </li> <li> <p>Update documentation if necessary</p> </li> </ol>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>We use MkDocs with the Material theme and mkdocstrings for documentation:</p> <ol> <li> <p>Install documentation dependencies:    <pre><code>pip install mkdocs mkdocs-material mkdocstrings mkdocstrings-python\n</code></pre></p> </li> <li> <p>Preview documentation locally:    <pre><code>mkdocs serve\n</code></pre></p> </li> <li> <p>Build documentation:    <pre><code>mkdocs build\n</code></pre></p> </li> </ol>"},{"location":"contributing/#submitting-changes","title":"Submitting Changes","text":"<ol> <li> <p>Commit your changes:    <pre><code>git commit -m \"Description of your changes\"\n</code></pre></p> </li> <li> <p>Push to your fork:    <pre><code>git push origin feature/your-feature-name\n</code></pre></p> </li> <li> <p>Submit a pull request on GitHub</p> </li> </ol>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Ensure all tests pass</li> <li>Update documentation if necessary</li> <li>The maintainers will review your PR</li> <li>Address any feedback and requested changes</li> <li>Once approved, your PR will be merged</li> </ol>"},{"location":"contributing/#code-style","title":"Code Style","text":"<ul> <li>Follow PEP 8 guidelines</li> <li>Use Google-style docstrings</li> <li>Use type hints where appropriate</li> </ul> <p>Thank you for contributing to BenchBBoxTest! </p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide will help you get started with BenchBBoxTest.</p> <p>For a quick overview and basic usage examples, please also refer to the main README.md file.</p>"},{"location":"getting-started/#installation","title":"Installation","text":"<pre><code># Clone the repository\ngit clone https://github.com/jiantingfeng/BenchBBoxTest.git\ncd BenchBBoxTest\n\n# Install dependencies\npip install -r requirements.txt\n\n# Install the package in development mode\npip install -e .\n</code></pre> <p>Alternatively, you can install the latest release directly using pip:</p> <pre><code>pip install benchbboxtest\n</code></pre>"},{"location":"getting-started/#basic-usage","title":"Basic Usage","text":"<p>Here's a basic example demonstrating how to use BenchBBoxTest for conditional independence testing:</p> <pre><code>import numpy as np\nfrom benchbboxtest.core import ConditionalRandomizationTest\nfrom benchbboxtest.datasets.simulation import LinearGaussianGenerator\nfrom benchbboxtest.evaluation import simultaneous_evaluation, plot_evaluation_results\n\n# Create a data generator\ndata_gen = LinearGaussianGenerator(d=5)\n\n# Create a conditional independence test\ncit = ConditionalRandomizationTest(n_permutations=100)\n\n# Evaluate the test across different sample sizes\nresults = simultaneous_evaluation(\n    test_method=cit,\n    data_generator=data_gen,\n    n_samples_list=[100, 200, 500, 1000],\n    n_trials=10\n)\n\n# Plot the evaluation results (e.g., power curve)\nplot_evaluation_results(results)\n</code></pre>"},{"location":"getting-started/#examples-with-different-data-types","title":"Examples with Different Data Types","text":"<p>BenchBBoxTest also supports benchmarking with other data modalities:</p> <ul> <li>Image Data: Use <code>CelebAMaskGenerator</code> (<code>benchbboxtest/datasets/image/celebamask.py</code>) for CIT tasks involving facial attributes from the CelebAMask-HQ dataset.</li> <li>Text Data: Use generators like <code>LLMGenerator</code> or <code>EHRTextGenerator</code> (<code>benchbboxtest/datasets/text/</code>) to create text-based scenarios. See the main <code>README.md</code> for a detailed example using <code>EHRTextGenerator</code>.</li> </ul>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<p>Refer to the User Guide and API Reference for more detailed information on the various modules and functionality provided by BenchBBoxTest. </p>"},{"location":"api/core/","title":"Core Module","text":"<p>This module provides the fundamental abstractions and base classes used throughout the package.</p>"},{"location":"api/core/#citest","title":"CITest","text":""},{"location":"api/core/#benchbboxtest.core.base.CITest","title":"<code>benchbboxtest.core.base.CITest</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all conditional independence test methods.</p> <p>All conditional independence tests should implement this interface. The test evaluates whether X \u22a5 Y | Z (X is independent of Y given Z).</p> Source code in <code>benchbboxtest/core/base.py</code> <pre><code>class CITest(ABC):\n    \"\"\"\n    Abstract base class for all conditional independence test methods.\n\n    All conditional independence tests should implement this interface.\n    The test evaluates whether X \u22a5 Y | Z (X is independent of Y given Z).\n    \"\"\"\n\n    @abstractmethod\n    def test(self, X: np.ndarray, Y: np.ndarray, Z: np.ndarray, \n             alpha: float = 0.05) -&gt; Tuple[bool, float]:\n        \"\"\"\n        Perform a conditional independence test.\n\n        Args:\n            X: The first variable\n            Y: The second variable\n            Z: The conditioning variable\n            alpha: Significance level\n\n        Returns:\n            Tuple containing:\n                - Boolean indicating rejection of the null hypothesis (True if X and Y are dependent given Z)\n                - p-value or test statistic\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/core/#benchbboxtest.core.base.CITest-functions","title":"Functions","text":""},{"location":"api/core/#benchbboxtest.core.base.CITest.test","title":"<code>test(X, Y, Z, alpha=0.05)</code>  <code>abstractmethod</code>","text":"<p>Perform a conditional independence test.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The first variable</p> required <code>Y</code> <code>ndarray</code> <p>The second variable</p> required <code>Z</code> <code>ndarray</code> <p>The conditioning variable</p> required <code>alpha</code> <code>float</code> <p>Significance level</p> <code>0.05</code> <p>Returns:</p> Type Description <code>Tuple[bool, float]</code> <p>Tuple containing: - Boolean indicating rejection of the null hypothesis (True if X and Y are dependent given Z) - p-value or test statistic</p> Source code in <code>benchbboxtest/core/base.py</code> <pre><code>@abstractmethod\ndef test(self, X: np.ndarray, Y: np.ndarray, Z: np.ndarray, \n         alpha: float = 0.05) -&gt; Tuple[bool, float]:\n    \"\"\"\n    Perform a conditional independence test.\n\n    Args:\n        X: The first variable\n        Y: The second variable\n        Z: The conditioning variable\n        alpha: Significance level\n\n    Returns:\n        Tuple containing:\n            - Boolean indicating rejection of the null hypothesis (True if X and Y are dependent given Z)\n            - p-value or test statistic\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#datagenerator","title":"DataGenerator","text":""},{"location":"api/core/#benchbboxtest.core.base.DataGenerator","title":"<code>benchbboxtest.core.base.DataGenerator</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for dataset generation.</p> <p>All data generators should implement this interface to provide consistent dataset generation across different modalities.</p> Source code in <code>benchbboxtest/core/base.py</code> <pre><code>class DataGenerator(ABC):\n    \"\"\"\n    Abstract base class for dataset generation.\n\n    All data generators should implement this interface to provide\n    consistent dataset generation across different modalities.\n    \"\"\"\n\n    @abstractmethod\n    def generate_null(self, n_samples: int, **kwargs) -&gt; Dict[str, np.ndarray]:\n        \"\"\"\n        Generate data under the null hypothesis (X \u22a5 Y | Z).\n\n        Args:\n            n_samples: Number of samples to generate\n            **kwargs: Additional parameters specific to the generator\n\n        Returns:\n            Dictionary containing 'X', 'Y', and 'Z' arrays\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def generate_alternative(self, n_samples: int, **kwargs) -&gt; Dict[str, np.ndarray]:\n        \"\"\"\n        Generate data under the alternative hypothesis (X \u22a5\u0338 Y | Z).\n\n        Args:\n            n_samples: Number of samples to generate\n            **kwargs: Additional parameters specific to the generator\n\n        Returns:\n            Dictionary containing 'X', 'Y', and 'Z' arrays\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/core/#benchbboxtest.core.base.DataGenerator-functions","title":"Functions","text":""},{"location":"api/core/#benchbboxtest.core.base.DataGenerator.generate_alternative","title":"<code>generate_alternative(n_samples, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Generate data under the alternative hypothesis (X \u22a5\u0338 Y | Z).</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Number of samples to generate</p> required <code>**kwargs</code> <p>Additional parameters specific to the generator</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>Dictionary containing 'X', 'Y', and 'Z' arrays</p> Source code in <code>benchbboxtest/core/base.py</code> <pre><code>@abstractmethod\ndef generate_alternative(self, n_samples: int, **kwargs) -&gt; Dict[str, np.ndarray]:\n    \"\"\"\n    Generate data under the alternative hypothesis (X \u22a5\u0338 Y | Z).\n\n    Args:\n        n_samples: Number of samples to generate\n        **kwargs: Additional parameters specific to the generator\n\n    Returns:\n        Dictionary containing 'X', 'Y', and 'Z' arrays\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#benchbboxtest.core.base.DataGenerator.generate_null","title":"<code>generate_null(n_samples, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Generate data under the null hypothesis (X \u22a5 Y | Z).</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Number of samples to generate</p> required <code>**kwargs</code> <p>Additional parameters specific to the generator</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>Dictionary containing 'X', 'Y', and 'Z' arrays</p> Source code in <code>benchbboxtest/core/base.py</code> <pre><code>@abstractmethod\ndef generate_null(self, n_samples: int, **kwargs) -&gt; Dict[str, np.ndarray]:\n    \"\"\"\n    Generate data under the null hypothesis (X \u22a5 Y | Z).\n\n    Args:\n        n_samples: Number of samples to generate\n        **kwargs: Additional parameters specific to the generator\n\n    Returns:\n        Dictionary containing 'X', 'Y', and 'Z' arrays\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#evaluator","title":"Evaluator","text":""},{"location":"api/core/#benchbboxtest.core.base.Evaluator","title":"<code>benchbboxtest.core.base.Evaluator</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for evaluation metrics.</p> <p>This class defines the interface for evaluating conditional independence tests.</p> Source code in <code>benchbboxtest/core/base.py</code> <pre><code>class Evaluator(ABC):\n    \"\"\"\n    Abstract base class for evaluation metrics.\n\n    This class defines the interface for evaluating conditional independence tests.\n    \"\"\"\n\n    @abstractmethod\n    def evaluate(self, test_results: Dict[str, Any]) -&gt; Dict[str, float]:\n        \"\"\"\n        Evaluate the performance of a conditional independence test.\n\n        Args:\n            test_results: Dictionary containing test results\n\n        Returns:\n            Dictionary of evaluation metrics\n        \"\"\"\n        pass \n</code></pre>"},{"location":"api/core/#benchbboxtest.core.base.Evaluator-functions","title":"Functions","text":""},{"location":"api/core/#benchbboxtest.core.base.Evaluator.evaluate","title":"<code>evaluate(test_results)</code>  <code>abstractmethod</code>","text":"<p>Evaluate the performance of a conditional independence test.</p> <p>Parameters:</p> Name Type Description Default <code>test_results</code> <code>Dict[str, Any]</code> <p>Dictionary containing test results</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dictionary of evaluation metrics</p> Source code in <code>benchbboxtest/core/base.py</code> <pre><code>@abstractmethod\ndef evaluate(self, test_results: Dict[str, Any]) -&gt; Dict[str, float]:\n    \"\"\"\n    Evaluate the performance of a conditional independence test.\n\n    Args:\n        test_results: Dictionary containing test results\n\n    Returns:\n        Dictionary of evaluation metrics\n    \"\"\"\n    pass \n</code></pre>"},{"location":"api/evaluation/","title":"Evaluation Module","text":"<p>This module provides metrics and evaluation tools to assess the performance of testing methods.</p>"},{"location":"api/evaluation/#simultaneous-evaluation","title":"Simultaneous Evaluation","text":""},{"location":"api/evaluation/#benchbboxtest.evaluation.metrics.simultaneous_evaluation","title":"<code>benchbboxtest.evaluation.metrics.simultaneous_evaluation(test_method, data_generator, n_samples_list, n_trials=100, alpha=0.05, **generator_kwargs)</code>","text":"<p>Implementation of Algorithm 1 from the paper: Simultaneous evaluation of Type I error and power.</p> <p>Parameters:</p> Name Type Description Default <code>test_method</code> <code>CITest</code> <p>The conditional independence test to evaluate</p> required <code>data_generator</code> <code>DataGenerator</code> <p>The data generator to use</p> required <code>n_samples_list</code> <code>List[int]</code> <p>List of sample sizes to evaluate</p> required <code>n_trials</code> <code>int</code> <p>Number of trials for each sample size</p> <code>100</code> <code>alpha</code> <code>float</code> <p>Significance level</p> <code>0.05</code> <code>**generator_kwargs</code> <p>Additional parameters for the data generator</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Dict[str, List[float]]]</code> <p>Dictionary containing evaluation results: - 'null': Dictionary with 'rejection_rates' for each sample size - 'alternative': Dictionary with 'rejection_rates' for each sample size</p> Source code in <code>benchbboxtest/evaluation/metrics.py</code> <pre><code>def simultaneous_evaluation(\n    test_method: CITest,\n    data_generator: DataGenerator,\n    n_samples_list: List[int],\n    n_trials: int = 100,\n    alpha: float = 0.05,\n    **generator_kwargs\n) -&gt; Dict[str, Dict[str, List[float]]]:\n    \"\"\"\n    Implementation of Algorithm 1 from the paper: Simultaneous evaluation of Type I error and power.\n\n    Args:\n        test_method: The conditional independence test to evaluate\n        data_generator: The data generator to use\n        n_samples_list: List of sample sizes to evaluate\n        n_trials: Number of trials for each sample size\n        alpha: Significance level\n        **generator_kwargs: Additional parameters for the data generator\n\n    Returns:\n        Dictionary containing evaluation results:\n            - 'null': Dictionary with 'rejection_rates' for each sample size\n            - 'alternative': Dictionary with 'rejection_rates' for each sample size\n    \"\"\"\n    results = {\n        'null': {'rejection_rates': [], 'sample_sizes': n_samples_list},\n        'alternative': {'rejection_rates': [], 'sample_sizes': n_samples_list}\n    }\n\n    for n_samples in n_samples_list:\n        # Type I error evaluation (null hypothesis)\n        null_rejections = 0\n        for _ in range(n_trials):\n            data = data_generator.generate_null(n_samples, **generator_kwargs)\n            reject, _ = test_method.test(data['X'], data['Y'], data['Z'], alpha=alpha)\n            if reject:\n                null_rejections += 1\n\n        type_I_error = null_rejections / n_trials\n        results['null']['rejection_rates'].append(type_I_error)\n\n        # Power evaluation (alternative hypothesis)\n        alt_rejections = 0\n        for _ in range(n_trials):\n            data = data_generator.generate_alternative(n_samples, **generator_kwargs)\n            reject, _ = test_method.test(data['X'], data['Y'], data['Z'], alpha=alpha)\n            if reject:\n                alt_rejections += 1\n\n        power = alt_rejections / n_trials\n        results['alternative']['rejection_rates'].append(power)\n\n    return results\n</code></pre>"},{"location":"api/evaluation/#plot-evaluation-results","title":"Plot Evaluation Results","text":""},{"location":"api/evaluation/#benchbboxtest.evaluation.metrics.plot_evaluation_results","title":"<code>benchbboxtest.evaluation.metrics.plot_evaluation_results(results, title='Conditional Independence Test Evaluation')</code>","text":"<p>Plot the evaluation results from simultaneous_evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>Dict[str, Dict[str, List[float]]]</code> <p>Results dictionary from simultaneous_evaluation</p> required <code>title</code> <code>str</code> <p>Plot title</p> <code>'Conditional Independence Test Evaluation'</code> Source code in <code>benchbboxtest/evaluation/metrics.py</code> <pre><code>def plot_evaluation_results(results: Dict[str, Dict[str, List[float]]], \n                           title: str = \"Conditional Independence Test Evaluation\"):\n    \"\"\"\n    Plot the evaluation results from simultaneous_evaluation.\n\n    Args:\n        results: Results dictionary from simultaneous_evaluation\n        title: Plot title\n    \"\"\"\n    plt.figure(figsize=(10, 6))\n\n    sample_sizes = results['null']['sample_sizes']\n    null_rates = results['null']['rejection_rates']\n    alt_rates = results['alternative']['rejection_rates']\n\n    plt.plot(sample_sizes, null_rates, 'o-', label='Type I Error (Null)')\n    plt.plot(sample_sizes, alt_rates, 's-', label='Power (Alternative)')\n    plt.axhline(y=0.05, color='r', linestyle='--', label='\u03b1 = 0.05')\n\n    plt.xlabel('Sample Size')\n    plt.ylabel('Rejection Rate')\n    plt.title(title)\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    return plt \n</code></pre>"},{"location":"api/utils/","title":"Utils Module","text":"<p>This module includes helper functions and utilities used across the package.</p>"},{"location":"api/utils/#visualization-functions","title":"Visualization Functions","text":""},{"location":"api/utils/#benchbboxtest.utils.visualization.plot_data_distribution","title":"<code>benchbboxtest.utils.visualization.plot_data_distribution(X, Y, Z=None, title='Data Distribution')</code>","text":"<p>Plot the distribution of the data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The first variable</p> required <code>Y</code> <code>ndarray</code> <p>The second variable</p> required <code>Z</code> <code>ndarray</code> <p>The conditioning variable (optional)</p> <code>None</code> <code>title</code> <code>str</code> <p>Plot title</p> <code>'Data Distribution'</code> Source code in <code>benchbboxtest/utils/visualization.py</code> <pre><code>def plot_data_distribution(X: np.ndarray, Y: np.ndarray, Z: np.ndarray = None, \n                          title: str = \"Data Distribution\"):\n    \"\"\"\n    Plot the distribution of the data.\n\n    Args:\n        X: The first variable\n        Y: The second variable\n        Z: The conditioning variable (optional)\n        title: Plot title\n    \"\"\"\n    plt.figure(figsize=(12, 8))\n\n    if Z is not None and Z.shape[1] &lt;= 2:\n        # If Z is 1D or 2D, we can visualize it\n        if Z.shape[1] == 1:\n            # 3D plot with X, Y, Z\n            ax = plt.subplot(111, projection='3d')\n            ax.scatter(X.ravel(), Y.ravel(), Z.ravel(), c=Y.ravel(), cmap='viridis', alpha=0.6)\n            ax.set_xlabel('X')\n            ax.set_ylabel('Y')\n            ax.set_zlabel('Z')\n        else:\n            # 3D plot with X, Y, Z[0], colored by Z[1]\n            ax = plt.subplot(111, projection='3d')\n            ax.scatter(X.ravel(), Y.ravel(), Z[:, 0], c=Z[:, 1], cmap='viridis', alpha=0.6)\n            ax.set_xlabel('X')\n            ax.set_ylabel('Y')\n            ax.set_zlabel('Z[0]')\n            plt.colorbar(ax.scatter(X.ravel(), Y.ravel(), Z[:, 0], c=Z[:, 1], cmap='viridis', alpha=0.6), \n                         label='Z[1]')\n    else:\n        # Simple 2D scatter plot of X vs Y\n        plt.scatter(X.ravel(), Y.ravel(), alpha=0.6)\n        plt.xlabel('X')\n        plt.ylabel('Y')\n\n    plt.title(title)\n    plt.tight_layout()\n\n    return plt\n</code></pre>"},{"location":"api/utils/#benchbboxtest.utils.visualization.plot_test_comparison","title":"<code>benchbboxtest.utils.visualization.plot_test_comparison(results_dict, title='Comparison of Conditional Independence Tests')</code>","text":"<p>Plot a comparison of multiple conditional independence tests.</p> <p>Parameters:</p> Name Type Description Default <code>results_dict</code> <code>Dict[str, Dict[str, Dict[str, List[float]]]]</code> <p>Dictionary mapping test names to results dictionaries from simultaneous_evaluation</p> required <code>title</code> <code>str</code> <p>Plot title</p> <code>'Comparison of Conditional Independence Tests'</code> Source code in <code>benchbboxtest/utils/visualization.py</code> <pre><code>def plot_test_comparison(results_dict: Dict[str, Dict[str, Dict[str, List[float]]]],\n                        title: str = \"Comparison of Conditional Independence Tests\"):\n    \"\"\"\n    Plot a comparison of multiple conditional independence tests.\n\n    Args:\n        results_dict: Dictionary mapping test names to results dictionaries from simultaneous_evaluation\n        title: Plot title\n    \"\"\"\n    plt.figure(figsize=(15, 10))\n\n    # Create subplots for Type I error and power\n    plt.subplot(1, 2, 1)\n    for test_name, results in results_dict.items():\n        sample_sizes = results['null']['sample_sizes']\n        null_rates = results['null']['rejection_rates']\n        plt.plot(sample_sizes, null_rates, 'o-', label=f'{test_name}')\n\n    plt.axhline(y=0.05, color='r', linestyle='--', label='\u03b1 = 0.05')\n    plt.xlabel('Sample Size')\n    plt.ylabel('Type I Error Rate')\n    plt.title('Type I Error Comparison')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    plt.subplot(1, 2, 2)\n    for test_name, results in results_dict.items():\n        sample_sizes = results['alternative']['sample_sizes']\n        alt_rates = results['alternative']['rejection_rates']\n        plt.plot(sample_sizes, alt_rates, 's-', label=f'{test_name}')\n\n    plt.xlabel('Sample Size')\n    plt.ylabel('Power')\n    plt.title('Power Comparison')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    plt.suptitle(title)\n    plt.tight_layout(rect=[0, 0, 1, 0.95])\n\n    return plt\n</code></pre>"},{"location":"api/utils/#benchbboxtest.utils.visualization.plot_heatmap","title":"<code>benchbboxtest.utils.visualization.plot_heatmap(matrix, row_labels, col_labels, title='Heatmap', cmap='viridis')</code>","text":"<p>Plot a heatmap of a matrix.</p> <p>Parameters:</p> Name Type Description Default <code>matrix</code> <code>ndarray</code> <p>Matrix to plot</p> required <code>row_labels</code> <code>List[str]</code> <p>Labels for the rows</p> required <code>col_labels</code> <code>List[str]</code> <p>Labels for the columns</p> required <code>title</code> <code>str</code> <p>Plot title</p> <code>'Heatmap'</code> <code>cmap</code> <code>str</code> <p>Colormap to use</p> <code>'viridis'</code> Source code in <code>benchbboxtest/utils/visualization.py</code> <pre><code>def plot_heatmap(matrix: np.ndarray, row_labels: List[str], col_labels: List[str],\n                title: str = \"Heatmap\", cmap: str = \"viridis\"):\n    \"\"\"\n    Plot a heatmap of a matrix.\n\n    Args:\n        matrix: Matrix to plot\n        row_labels: Labels for the rows\n        col_labels: Labels for the columns\n        title: Plot title\n        cmap: Colormap to use\n    \"\"\"\n    plt.figure(figsize=(10, 8))\n\n    sns.heatmap(matrix, annot=True, cmap=cmap, xticklabels=col_labels, yticklabels=row_labels)\n\n    plt.title(title)\n    plt.tight_layout()\n\n    return plt \n</code></pre>"},{"location":"api/datasets/image/","title":"Image Dataset Module","text":"<p>This module provides tools for generating image datasets for testing purposes.</p>"},{"location":"api/datasets/image/#celebamaskgenerator","title":"CelebAMaskGenerator","text":""},{"location":"api/datasets/image/#benchbboxtest.datasets.image.celebamask.CelebAMaskGenerator","title":"<code>benchbboxtest.datasets.image.celebamask.CelebAMaskGenerator</code>","text":"<p>               Bases: <code>DataGenerator</code></p> <p>Data generator for CelebAMask-HQ dataset.</p> <p>Implements data generation for conditional independence testing using facial attributes and regions.</p> Source code in <code>benchbboxtest/datasets/image/celebamask.py</code> <pre><code>class CelebAMaskGenerator(DataGenerator):\n    \"\"\"\n    Data generator for CelebAMask-HQ dataset.\n\n    Implements data generation for conditional independence testing using facial attributes and regions.\n    \"\"\"\n\n    # Mapping from attributes to facial regions\n    ATTRIBUTE_REGION_MAP = {\n        'Narrow_Eyes': 'eye',\n        'Pointy_Nose': 'nose',\n        'Big_Lips': 'mouth',\n        'Bushy_Eyebrows': 'eyebrow',\n        'Male': 'face',\n        'Smiling': 'mouth',\n        'Wearing_Earrings': 'ear',\n        'Wearing_Hat': 'hair',\n        'Wearing_Lipstick': 'mouth',\n        'Wearing_Necklace': 'neck'\n    }\n\n    def __init__(self, dataset_path: str, attribute: str, crucial_region: str = None):\n        \"\"\"\n        Initialize the CelebAMask-HQ generator.\n\n        Args:\n            dataset_path: Path to the CelebAMask-HQ dataset\n            attribute: Facial attribute to predict (Y)\n            crucial_region: Crucial region for the attribute (if None, determined from ATTRIBUTE_REGION_MAP)\n        \"\"\"\n        self.dataset_path = dataset_path\n        self.attribute = attribute\n\n        if crucial_region is None and attribute in self.ATTRIBUTE_REGION_MAP:\n            self.crucial_region = self.ATTRIBUTE_REGION_MAP[attribute]\n        else:\n            self.crucial_region = crucial_region\n\n    def generate_null(self, n_samples: int, **kwargs) -&gt; Dict[str, np.ndarray]:\n        \"\"\"\n        Generate data under the null hypothesis (X \u22a5 Y | Z).\n\n        For the null hypothesis, we mask non-crucial regions, making X and Y conditionally independent given Z.\n\n        Args:\n            n_samples: Number of samples to generate\n            **kwargs: Additional parameters\n\n        Returns:\n            Dictionary containing 'X', 'Y', and 'Z' arrays\n        \"\"\"\n        # This is a simplified implementation\n        # In a real implementation, you would:\n        # 1. Load n_samples images from the dataset\n        # 2. Extract the attribute values (Y)\n        # 3. Extract the crucial region features (Z)\n        # 4. Mask non-crucial regions to create X\n\n        # Placeholder implementation\n        X = np.random.rand(n_samples, 64, 64, 3)  # Masked images (non-crucial regions)\n        Y = np.random.randint(0, 2, size=n_samples)  # Binary attribute values\n        Z = np.random.rand(n_samples, 32)  # Features from crucial regions\n\n        return {'X': X, 'Y': Y.reshape(-1, 1), 'Z': Z}\n\n    def generate_alternative(self, n_samples: int, **kwargs) -&gt; Dict[str, np.ndarray]:\n        \"\"\"\n        Generate data under the alternative hypothesis (X \u22a5\u0338 Y | Z).\n\n        For the alternative hypothesis, we mask crucial regions, making X and Y conditionally dependent given Z.\n\n        Args:\n            n_samples: Number of samples to generate\n            **kwargs: Additional parameters\n\n        Returns:\n            Dictionary containing 'X', 'Y', and 'Z' arrays\n        \"\"\"\n        # This is a simplified implementation\n        # In a real implementation, you would:\n        # 1. Load n_samples images from the dataset\n        # 2. Extract the attribute values (Y)\n        # 3. Extract features from non-crucial regions (Z)\n        # 4. Mask crucial regions to create X\n\n        # Placeholder implementation\n        X = np.random.rand(n_samples, 64, 64, 3)  # Masked images (crucial regions)\n        Y = np.random.randint(0, 2, size=n_samples)  # Binary attribute values\n        Z = np.random.rand(n_samples, 32)  # Features from non-crucial regions\n\n        return {'X': X, 'Y': Y.reshape(-1, 1), 'Z': Z} \n</code></pre>"},{"location":"api/datasets/image/#benchbboxtest.datasets.image.celebamask.CelebAMaskGenerator-functions","title":"Functions","text":""},{"location":"api/datasets/image/#benchbboxtest.datasets.image.celebamask.CelebAMaskGenerator.__init__","title":"<code>__init__(dataset_path, attribute, crucial_region=None)</code>","text":"<p>Initialize the CelebAMask-HQ generator.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>Path to the CelebAMask-HQ dataset</p> required <code>attribute</code> <code>str</code> <p>Facial attribute to predict (Y)</p> required <code>crucial_region</code> <code>str</code> <p>Crucial region for the attribute (if None, determined from ATTRIBUTE_REGION_MAP)</p> <code>None</code> Source code in <code>benchbboxtest/datasets/image/celebamask.py</code> <pre><code>def __init__(self, dataset_path: str, attribute: str, crucial_region: str = None):\n    \"\"\"\n    Initialize the CelebAMask-HQ generator.\n\n    Args:\n        dataset_path: Path to the CelebAMask-HQ dataset\n        attribute: Facial attribute to predict (Y)\n        crucial_region: Crucial region for the attribute (if None, determined from ATTRIBUTE_REGION_MAP)\n    \"\"\"\n    self.dataset_path = dataset_path\n    self.attribute = attribute\n\n    if crucial_region is None and attribute in self.ATTRIBUTE_REGION_MAP:\n        self.crucial_region = self.ATTRIBUTE_REGION_MAP[attribute]\n    else:\n        self.crucial_region = crucial_region\n</code></pre>"},{"location":"api/datasets/image/#benchbboxtest.datasets.image.celebamask.CelebAMaskGenerator.generate_alternative","title":"<code>generate_alternative(n_samples, **kwargs)</code>","text":"<p>Generate data under the alternative hypothesis (X \u22a5\u0338 Y | Z).</p> <p>For the alternative hypothesis, we mask crucial regions, making X and Y conditionally dependent given Z.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Number of samples to generate</p> required <code>**kwargs</code> <p>Additional parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>Dictionary containing 'X', 'Y', and 'Z' arrays</p> Source code in <code>benchbboxtest/datasets/image/celebamask.py</code> <pre><code>def generate_alternative(self, n_samples: int, **kwargs) -&gt; Dict[str, np.ndarray]:\n    \"\"\"\n    Generate data under the alternative hypothesis (X \u22a5\u0338 Y | Z).\n\n    For the alternative hypothesis, we mask crucial regions, making X and Y conditionally dependent given Z.\n\n    Args:\n        n_samples: Number of samples to generate\n        **kwargs: Additional parameters\n\n    Returns:\n        Dictionary containing 'X', 'Y', and 'Z' arrays\n    \"\"\"\n    # This is a simplified implementation\n    # In a real implementation, you would:\n    # 1. Load n_samples images from the dataset\n    # 2. Extract the attribute values (Y)\n    # 3. Extract features from non-crucial regions (Z)\n    # 4. Mask crucial regions to create X\n\n    # Placeholder implementation\n    X = np.random.rand(n_samples, 64, 64, 3)  # Masked images (crucial regions)\n    Y = np.random.randint(0, 2, size=n_samples)  # Binary attribute values\n    Z = np.random.rand(n_samples, 32)  # Features from non-crucial regions\n\n    return {'X': X, 'Y': Y.reshape(-1, 1), 'Z': Z} \n</code></pre>"},{"location":"api/datasets/image/#benchbboxtest.datasets.image.celebamask.CelebAMaskGenerator.generate_null","title":"<code>generate_null(n_samples, **kwargs)</code>","text":"<p>Generate data under the null hypothesis (X \u22a5 Y | Z).</p> <p>For the null hypothesis, we mask non-crucial regions, making X and Y conditionally independent given Z.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Number of samples to generate</p> required <code>**kwargs</code> <p>Additional parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>Dictionary containing 'X', 'Y', and 'Z' arrays</p> Source code in <code>benchbboxtest/datasets/image/celebamask.py</code> <pre><code>def generate_null(self, n_samples: int, **kwargs) -&gt; Dict[str, np.ndarray]:\n    \"\"\"\n    Generate data under the null hypothesis (X \u22a5 Y | Z).\n\n    For the null hypothesis, we mask non-crucial regions, making X and Y conditionally independent given Z.\n\n    Args:\n        n_samples: Number of samples to generate\n        **kwargs: Additional parameters\n\n    Returns:\n        Dictionary containing 'X', 'Y', and 'Z' arrays\n    \"\"\"\n    # This is a simplified implementation\n    # In a real implementation, you would:\n    # 1. Load n_samples images from the dataset\n    # 2. Extract the attribute values (Y)\n    # 3. Extract the crucial region features (Z)\n    # 4. Mask non-crucial regions to create X\n\n    # Placeholder implementation\n    X = np.random.rand(n_samples, 64, 64, 3)  # Masked images (non-crucial regions)\n    Y = np.random.randint(0, 2, size=n_samples)  # Binary attribute values\n    Z = np.random.rand(n_samples, 32)  # Features from crucial regions\n\n    return {'X': X, 'Y': Y.reshape(-1, 1), 'Z': Z}\n</code></pre>"},{"location":"api/datasets/image/#celebamaskdataset","title":"CelebAMaskDataset","text":""},{"location":"api/datasets/image/#benchbboxtest.datasets.image.celebamask.CelebAMaskDataset","title":"<code>benchbboxtest.datasets.image.celebamask.CelebAMaskDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset class for CelebAMask-HQ dataset.</p> Source code in <code>benchbboxtest/datasets/image/celebamask.py</code> <pre><code>class CelebAMaskDataset(Dataset):\n    \"\"\"\n    Dataset class for CelebAMask-HQ dataset.\n    \"\"\"\n\n    def __init__(self, root_dir: str, transform=None):\n        \"\"\"\n        Initialize the CelebAMask-HQ dataset.\n\n        Args:\n            root_dir: Root directory containing the dataset\n            transform: Optional transform to be applied to the images\n        \"\"\"\n        self.root_dir = root_dir\n        self.transform = transform\n        self.image_dir = os.path.join(root_dir, 'CelebA-HQ-img')\n        self.mask_dir = os.path.join(root_dir, 'CelebAMask-HQ-mask-anno')\n\n        # Get list of image files\n        self.image_files = sorted([f for f in os.listdir(self.image_dir) \n                                  if f.endswith('.jpg') or f.endswith('.png')])\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        img_name = os.path.join(self.image_dir, self.image_files[idx])\n        image = Image.open(img_name).convert('RGB')\n\n        # Get corresponding mask files\n        img_id = self.image_files[idx].split('.')[0]\n        mask_files = [f for f in os.listdir(self.mask_dir) if f.startswith(img_id)]\n\n        # Load masks\n        masks = {}\n        for mask_file in mask_files:\n            region_name = mask_file.split('_')[1]\n            mask_path = os.path.join(self.mask_dir, mask_file)\n            mask = Image.open(mask_path).convert('L')\n            masks[region_name] = np.array(mask)\n\n        if self.transform:\n            image = self.transform(image)\n\n        return {'image': image, 'masks': masks, 'id': img_id}\n</code></pre>"},{"location":"api/datasets/image/#benchbboxtest.datasets.image.celebamask.CelebAMaskDataset-functions","title":"Functions","text":""},{"location":"api/datasets/image/#benchbboxtest.datasets.image.celebamask.CelebAMaskDataset.__init__","title":"<code>__init__(root_dir, transform=None)</code>","text":"<p>Initialize the CelebAMask-HQ dataset.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>Root directory containing the dataset</p> required <code>transform</code> <p>Optional transform to be applied to the images</p> <code>None</code> Source code in <code>benchbboxtest/datasets/image/celebamask.py</code> <pre><code>def __init__(self, root_dir: str, transform=None):\n    \"\"\"\n    Initialize the CelebAMask-HQ dataset.\n\n    Args:\n        root_dir: Root directory containing the dataset\n        transform: Optional transform to be applied to the images\n    \"\"\"\n    self.root_dir = root_dir\n    self.transform = transform\n    self.image_dir = os.path.join(root_dir, 'CelebA-HQ-img')\n    self.mask_dir = os.path.join(root_dir, 'CelebAMask-HQ-mask-anno')\n\n    # Get list of image files\n    self.image_files = sorted([f for f in os.listdir(self.image_dir) \n                              if f.endswith('.jpg') or f.endswith('.png')])\n</code></pre>"},{"location":"api/datasets/image/#helper-functions","title":"Helper Functions","text":""},{"location":"api/datasets/image/#benchbboxtest.datasets.image.celebamask.download_celebamask_hq","title":"<code>benchbboxtest.datasets.image.celebamask.download_celebamask_hq(target_dir)</code>","text":"<p>Download and extract the CelebAMask-HQ dataset.</p> <p>Parameters:</p> Name Type Description Default <code>target_dir</code> <code>str</code> <p>Directory to save the dataset</p> required Source code in <code>benchbboxtest/datasets/image/celebamask.py</code> <pre><code>def download_celebamask_hq(target_dir: str):\n    \"\"\"\n    Download and extract the CelebAMask-HQ dataset.\n\n    Args:\n        target_dir: Directory to save the dataset\n    \"\"\"\n    # This is a placeholder - in a real implementation, you would need to:\n    # 1. Download the dataset from the official source\n    # 2. Extract it to the target directory\n    # 3. Organize the files as needed\n\n    os.makedirs(target_dir, exist_ok=True)\n    print(f\"CelebAMask-HQ dataset would be downloaded to {target_dir}\")\n    print(\"Note: This is a placeholder. In a real implementation, you would need to download the dataset from the official source.\")\n    print(\"The CelebAMask-HQ dataset is available at: https://github.com/switchablenorms/CelebAMask-HQ\")\n</code></pre>"},{"location":"api/datasets/image/#benchbboxtest.datasets.image.celebamask.mask_region","title":"<code>benchbboxtest.datasets.image.celebamask.mask_region(image, segmentation_mask, value=0)</code>","text":"<p>Apply a mask to a specific region of an image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>Input image (H, W, C)</p> required <code>segmentation_mask</code> <code>ndarray</code> <p>Binary mask for the region (H, W)</p> required <code>value</code> <code>Union[int, Tuple[int, int, int]]</code> <p>Value to fill the masked region with</p> <code>0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Masked image</p> Source code in <code>benchbboxtest/datasets/image/celebamask.py</code> <pre><code>def mask_region(image: np.ndarray, segmentation_mask: np.ndarray, \n                value: Union[int, Tuple[int, int, int]] = 0) -&gt; np.ndarray:\n    \"\"\"\n    Apply a mask to a specific region of an image.\n\n    Args:\n        image: Input image (H, W, C)\n        segmentation_mask: Binary mask for the region (H, W)\n        value: Value to fill the masked region with\n\n    Returns:\n        Masked image\n    \"\"\"\n    masked_image = image.copy()\n\n    if len(image.shape) == 3:  # RGB image\n        for c in range(image.shape[2]):\n            masked_image[:, :, c] = np.where(segmentation_mask &gt; 0, value if isinstance(value, int) else value[c], image[:, :, c])\n    else:  # Grayscale image\n        masked_image = np.where(segmentation_mask &gt; 0, value, image)\n\n    return masked_image\n</code></pre>"},{"location":"api/datasets/simulation/","title":"Simulation Dataset Module","text":"<p>This module provides tools for simulating data from various distributions.</p>"},{"location":"api/datasets/simulation/#lineargaussiangenerator","title":"LinearGaussianGenerator","text":""},{"location":"api/datasets/simulation/#benchbboxtest.datasets.simulation.linear_gaussian.LinearGaussianGenerator","title":"<code>benchbboxtest.datasets.simulation.linear_gaussian.LinearGaussianGenerator</code>","text":"<p>               Bases: <code>DataGenerator</code></p> <p>Linear Gaussian models from Shah and Peters (2020).</p> <p>Implements data generation for linear Gaussian models under both null and alternative hypotheses. The models follow the structure:</p> <p>Under null hypothesis (X \u22a5 Y | Z): - Z ~ N(0, I_d) - X = Z^T \u03b2_x + \u03b5_x, where \u03b5_x ~ N(0, \u03c3\u00b2) - Y = Z^T \u03b2_y + \u03b5_y, where \u03b5_y ~ N(0, \u03c3\u00b2)</p> <p>Under alternative hypothesis (X \u22a5\u0338 Y | Z): - Z ~ N(0, I_d) - X = Z^T \u03b2_x + \u03b5_x, where \u03b5_x ~ N(0, \u03c3\u00b2) - Y = Z^T \u03b2_y + \u03b3 * X + \u03b5_y, where \u03b5_y ~ N(0, \u03c3\u00b2)</p> <p>where: - d is the dimension of Z - \u03b2_x, \u03b2_y are coefficient vectors - \u03b3 controls the strength of dependence between X and Y - \u03c3\u00b2 is the noise variance</p> Source code in <code>benchbboxtest/datasets/simulation/linear_gaussian.py</code> <pre><code>class LinearGaussianGenerator(DataGenerator):\n    \"\"\"\n    Linear Gaussian models from Shah and Peters (2020).\n\n    Implements data generation for linear Gaussian models under both\n    null and alternative hypotheses. The models follow the structure:\n\n    Under null hypothesis (X \u22a5 Y | Z):\n    - Z ~ N(0, I_d)\n    - X = Z^T \u03b2_x + \u03b5_x, where \u03b5_x ~ N(0, \u03c3\u00b2)\n    - Y = Z^T \u03b2_y + \u03b5_y, where \u03b5_y ~ N(0, \u03c3\u00b2)\n\n    Under alternative hypothesis (X \u22a5\u0338 Y | Z):\n    - Z ~ N(0, I_d)\n    - X = Z^T \u03b2_x + \u03b5_x, where \u03b5_x ~ N(0, \u03c3\u00b2)\n    - Y = Z^T \u03b2_y + \u03b3 * X + \u03b5_y, where \u03b5_y ~ N(0, \u03c3\u00b2)\n\n    where:\n    - d is the dimension of Z\n    - \u03b2_x, \u03b2_y are coefficient vectors\n    - \u03b3 controls the strength of dependence between X and Y\n    - \u03c3\u00b2 is the noise variance\n    \"\"\"\n\n    def __init__(\n        self,\n        d: int = 5,\n        beta_x: Optional[np.ndarray] = None,\n        beta_y: Optional[np.ndarray] = None,\n        covariance_z: Optional[np.ndarray] = None,\n    ):\n        \"\"\"\n        Initialize the linear Gaussian generator.\n\n        Args:\n            d: Dimension of the conditioning variable Z\n            beta_x: Coefficients for X ~ Z relationship (if None, random values are generated)\n            beta_y: Coefficients for Y ~ Z relationship (if None, random values are generated)\n            covariance_z: Covariance matrix for Z (if None, identity matrix is used)\n        \"\"\"\n        self.d = d\n\n        # Initialize coefficients if not provided\n        if beta_x is None:\n            self.beta_x = np.random.normal(0, 1, size=d)\n        else:\n            self.beta_x = beta_x\n\n        if beta_y is None:\n            self.beta_y = np.random.normal(0, 1, size=d)\n        else:\n            self.beta_y = beta_y\n\n        # Initialize covariance matrix for Z\n        if covariance_z is None:\n            self.covariance_z = np.eye(d)  # Identity matrix\n        else:\n            # Ensure the covariance matrix is valid (symmetric and positive semi-definite)\n            if covariance_z.shape != (d, d):\n                raise ValueError(f\"Covariance matrix must have shape ({d}, {d})\")\n            self.covariance_z = covariance_z\n\n    def generate_null(\n        self, n_samples: int, noise_scale: float = 1.0, return_params: bool = False\n    ) -&gt; Dict[str, Union[np.ndarray, Dict]]:\n        \"\"\"\n        Generate data under the null hypothesis (X \u22a5 Y | Z).\n\n        Args:\n            n_samples: Number of samples to generate\n            noise_scale: Scale of the noise (standard deviation)\n            return_params: Whether to return the parameters used for generation\n\n        Returns:\n            Dictionary containing 'X', 'Y', and 'Z' arrays, and optionally 'params'\n        \"\"\"\n        # Generate Z ~ N(0, \u03a3)\n        Z = np.random.multivariate_normal(\n            mean=np.zeros(self.d), cov=self.covariance_z, size=n_samples\n        )\n\n        # Generate X = Z * beta_x + epsilon_x\n        epsilon_x = np.random.normal(0, noise_scale, size=n_samples)\n        X = Z @ self.beta_x + epsilon_x\n\n        # Generate Y = Z * beta_y + epsilon_y (independent of X given Z)\n        epsilon_y = np.random.normal(0, noise_scale, size=n_samples)\n        Y = Z @ self.beta_y + epsilon_y\n\n        result = {\"X\": X.reshape(-1, 1), \"Y\": Y.reshape(-1, 1), \"Z\": Z}\n\n        if return_params:\n            params = {\n                \"beta_x\": self.beta_x,\n                \"beta_y\": self.beta_y,\n                \"noise_scale\": noise_scale,\n                \"covariance_z\": self.covariance_z,\n                \"hypothesis\": \"null\",\n            }\n            result[\"params\"] = params\n\n        return result\n\n    def generate_alternative(\n        self,\n        n_samples: int,\n        gamma: float = 0.5,\n        noise_scale: float = 1.0,\n        return_params: bool = False,\n    ) -&gt; Dict[str, Union[np.ndarray, Dict]]:\n        \"\"\"\n        Generate data under the alternative hypothesis (X \u22a5\u0338 Y | Z).\n\n        Args:\n            n_samples: Number of samples to generate\n            gamma: Strength of dependence between X and Y\n            noise_scale: Scale of the noise (standard deviation)\n            return_params: Whether to return the parameters used for generation\n\n        Returns:\n            Dictionary containing 'X', 'Y', and 'Z' arrays, and optionally 'params'\n        \"\"\"\n        # Generate Z ~ N(0, \u03a3)\n        Z = np.random.multivariate_normal(\n            mean=np.zeros(self.d), cov=self.covariance_z, size=n_samples\n        )\n\n        # Generate X = Z * beta_x + epsilon_x\n        epsilon_x = np.random.normal(0, noise_scale, size=n_samples)\n        X = Z @ self.beta_x + epsilon_x\n\n        # Generate Y = Z * beta_y + gamma * X + epsilon_y (dependent on X given Z)\n        epsilon_y = np.random.normal(0, noise_scale, size=n_samples)\n        Y = Z @ self.beta_y + gamma * X + epsilon_y\n\n        result = {\"X\": X.reshape(-1, 1), \"Y\": Y.reshape(-1, 1), \"Z\": Z}\n\n        if return_params:\n            params = {\n                \"beta_x\": self.beta_x,\n                \"beta_y\": self.beta_y,\n                \"gamma\": gamma,\n                \"noise_scale\": noise_scale,\n                \"covariance_z\": self.covariance_z,\n                \"hypothesis\": \"alternative\",\n            }\n            result[\"params\"] = params\n\n        return result\n\n    def generate_dataset(\n        self,\n        n_samples: int,\n        hypothesis: str = \"null\",\n        gamma: float = 0.5,\n        noise_scale: float = 1.0,\n        return_params: bool = False,\n    ) -&gt; Dict[str, Union[np.ndarray, Dict]]:\n        \"\"\"\n        Generate a dataset under either hypothesis.\n\n        Args:\n            n_samples: Number of samples to generate\n            hypothesis: 'null' or 'alternative'\n            gamma: Strength of dependence between X and Y (for alternative)\n            noise_scale: Scale of the noise\n            return_params: Whether to return the parameters used for generation\n\n        Returns:\n            Dictionary containing generated data\n        \"\"\"\n        if hypothesis.lower() == \"null\":\n            return self.generate_null(n_samples, noise_scale, return_params)\n        elif hypothesis.lower() == \"alternative\":\n            return self.generate_alternative(\n                n_samples, gamma, noise_scale, return_params\n            )\n        else:\n            raise ValueError(\"hypothesis must be 'null' or 'alternative'\")\n</code></pre>"},{"location":"api/datasets/simulation/#benchbboxtest.datasets.simulation.linear_gaussian.LinearGaussianGenerator-functions","title":"Functions","text":""},{"location":"api/datasets/simulation/#benchbboxtest.datasets.simulation.linear_gaussian.LinearGaussianGenerator.__init__","title":"<code>__init__(d=5, beta_x=None, beta_y=None, covariance_z=None)</code>","text":"<p>Initialize the linear Gaussian generator.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>int</code> <p>Dimension of the conditioning variable Z</p> <code>5</code> <code>beta_x</code> <code>Optional[ndarray]</code> <p>Coefficients for X ~ Z relationship (if None, random values are generated)</p> <code>None</code> <code>beta_y</code> <code>Optional[ndarray]</code> <p>Coefficients for Y ~ Z relationship (if None, random values are generated)</p> <code>None</code> <code>covariance_z</code> <code>Optional[ndarray]</code> <p>Covariance matrix for Z (if None, identity matrix is used)</p> <code>None</code> Source code in <code>benchbboxtest/datasets/simulation/linear_gaussian.py</code> <pre><code>def __init__(\n    self,\n    d: int = 5,\n    beta_x: Optional[np.ndarray] = None,\n    beta_y: Optional[np.ndarray] = None,\n    covariance_z: Optional[np.ndarray] = None,\n):\n    \"\"\"\n    Initialize the linear Gaussian generator.\n\n    Args:\n        d: Dimension of the conditioning variable Z\n        beta_x: Coefficients for X ~ Z relationship (if None, random values are generated)\n        beta_y: Coefficients for Y ~ Z relationship (if None, random values are generated)\n        covariance_z: Covariance matrix for Z (if None, identity matrix is used)\n    \"\"\"\n    self.d = d\n\n    # Initialize coefficients if not provided\n    if beta_x is None:\n        self.beta_x = np.random.normal(0, 1, size=d)\n    else:\n        self.beta_x = beta_x\n\n    if beta_y is None:\n        self.beta_y = np.random.normal(0, 1, size=d)\n    else:\n        self.beta_y = beta_y\n\n    # Initialize covariance matrix for Z\n    if covariance_z is None:\n        self.covariance_z = np.eye(d)  # Identity matrix\n    else:\n        # Ensure the covariance matrix is valid (symmetric and positive semi-definite)\n        if covariance_z.shape != (d, d):\n            raise ValueError(f\"Covariance matrix must have shape ({d}, {d})\")\n        self.covariance_z = covariance_z\n</code></pre>"},{"location":"api/datasets/simulation/#benchbboxtest.datasets.simulation.linear_gaussian.LinearGaussianGenerator.generate_alternative","title":"<code>generate_alternative(n_samples, gamma=0.5, noise_scale=1.0, return_params=False)</code>","text":"<p>Generate data under the alternative hypothesis (X \u22a5\u0338 Y | Z).</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Number of samples to generate</p> required <code>gamma</code> <code>float</code> <p>Strength of dependence between X and Y</p> <code>0.5</code> <code>noise_scale</code> <code>float</code> <p>Scale of the noise (standard deviation)</p> <code>1.0</code> <code>return_params</code> <code>bool</code> <p>Whether to return the parameters used for generation</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Union[ndarray, Dict]]</code> <p>Dictionary containing 'X', 'Y', and 'Z' arrays, and optionally 'params'</p> Source code in <code>benchbboxtest/datasets/simulation/linear_gaussian.py</code> <pre><code>def generate_alternative(\n    self,\n    n_samples: int,\n    gamma: float = 0.5,\n    noise_scale: float = 1.0,\n    return_params: bool = False,\n) -&gt; Dict[str, Union[np.ndarray, Dict]]:\n    \"\"\"\n    Generate data under the alternative hypothesis (X \u22a5\u0338 Y | Z).\n\n    Args:\n        n_samples: Number of samples to generate\n        gamma: Strength of dependence between X and Y\n        noise_scale: Scale of the noise (standard deviation)\n        return_params: Whether to return the parameters used for generation\n\n    Returns:\n        Dictionary containing 'X', 'Y', and 'Z' arrays, and optionally 'params'\n    \"\"\"\n    # Generate Z ~ N(0, \u03a3)\n    Z = np.random.multivariate_normal(\n        mean=np.zeros(self.d), cov=self.covariance_z, size=n_samples\n    )\n\n    # Generate X = Z * beta_x + epsilon_x\n    epsilon_x = np.random.normal(0, noise_scale, size=n_samples)\n    X = Z @ self.beta_x + epsilon_x\n\n    # Generate Y = Z * beta_y + gamma * X + epsilon_y (dependent on X given Z)\n    epsilon_y = np.random.normal(0, noise_scale, size=n_samples)\n    Y = Z @ self.beta_y + gamma * X + epsilon_y\n\n    result = {\"X\": X.reshape(-1, 1), \"Y\": Y.reshape(-1, 1), \"Z\": Z}\n\n    if return_params:\n        params = {\n            \"beta_x\": self.beta_x,\n            \"beta_y\": self.beta_y,\n            \"gamma\": gamma,\n            \"noise_scale\": noise_scale,\n            \"covariance_z\": self.covariance_z,\n            \"hypothesis\": \"alternative\",\n        }\n        result[\"params\"] = params\n\n    return result\n</code></pre>"},{"location":"api/datasets/simulation/#benchbboxtest.datasets.simulation.linear_gaussian.LinearGaussianGenerator.generate_dataset","title":"<code>generate_dataset(n_samples, hypothesis='null', gamma=0.5, noise_scale=1.0, return_params=False)</code>","text":"<p>Generate a dataset under either hypothesis.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Number of samples to generate</p> required <code>hypothesis</code> <code>str</code> <p>'null' or 'alternative'</p> <code>'null'</code> <code>gamma</code> <code>float</code> <p>Strength of dependence between X and Y (for alternative)</p> <code>0.5</code> <code>noise_scale</code> <code>float</code> <p>Scale of the noise</p> <code>1.0</code> <code>return_params</code> <code>bool</code> <p>Whether to return the parameters used for generation</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Union[ndarray, Dict]]</code> <p>Dictionary containing generated data</p> Source code in <code>benchbboxtest/datasets/simulation/linear_gaussian.py</code> <pre><code>def generate_dataset(\n    self,\n    n_samples: int,\n    hypothesis: str = \"null\",\n    gamma: float = 0.5,\n    noise_scale: float = 1.0,\n    return_params: bool = False,\n) -&gt; Dict[str, Union[np.ndarray, Dict]]:\n    \"\"\"\n    Generate a dataset under either hypothesis.\n\n    Args:\n        n_samples: Number of samples to generate\n        hypothesis: 'null' or 'alternative'\n        gamma: Strength of dependence between X and Y (for alternative)\n        noise_scale: Scale of the noise\n        return_params: Whether to return the parameters used for generation\n\n    Returns:\n        Dictionary containing generated data\n    \"\"\"\n    if hypothesis.lower() == \"null\":\n        return self.generate_null(n_samples, noise_scale, return_params)\n    elif hypothesis.lower() == \"alternative\":\n        return self.generate_alternative(\n            n_samples, gamma, noise_scale, return_params\n        )\n    else:\n        raise ValueError(\"hypothesis must be 'null' or 'alternative'\")\n</code></pre>"},{"location":"api/datasets/simulation/#benchbboxtest.datasets.simulation.linear_gaussian.LinearGaussianGenerator.generate_null","title":"<code>generate_null(n_samples, noise_scale=1.0, return_params=False)</code>","text":"<p>Generate data under the null hypothesis (X \u22a5 Y | Z).</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Number of samples to generate</p> required <code>noise_scale</code> <code>float</code> <p>Scale of the noise (standard deviation)</p> <code>1.0</code> <code>return_params</code> <code>bool</code> <p>Whether to return the parameters used for generation</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Union[ndarray, Dict]]</code> <p>Dictionary containing 'X', 'Y', and 'Z' arrays, and optionally 'params'</p> Source code in <code>benchbboxtest/datasets/simulation/linear_gaussian.py</code> <pre><code>def generate_null(\n    self, n_samples: int, noise_scale: float = 1.0, return_params: bool = False\n) -&gt; Dict[str, Union[np.ndarray, Dict]]:\n    \"\"\"\n    Generate data under the null hypothesis (X \u22a5 Y | Z).\n\n    Args:\n        n_samples: Number of samples to generate\n        noise_scale: Scale of the noise (standard deviation)\n        return_params: Whether to return the parameters used for generation\n\n    Returns:\n        Dictionary containing 'X', 'Y', and 'Z' arrays, and optionally 'params'\n    \"\"\"\n    # Generate Z ~ N(0, \u03a3)\n    Z = np.random.multivariate_normal(\n        mean=np.zeros(self.d), cov=self.covariance_z, size=n_samples\n    )\n\n    # Generate X = Z * beta_x + epsilon_x\n    epsilon_x = np.random.normal(0, noise_scale, size=n_samples)\n    X = Z @ self.beta_x + epsilon_x\n\n    # Generate Y = Z * beta_y + epsilon_y (independent of X given Z)\n    epsilon_y = np.random.normal(0, noise_scale, size=n_samples)\n    Y = Z @ self.beta_y + epsilon_y\n\n    result = {\"X\": X.reshape(-1, 1), \"Y\": Y.reshape(-1, 1), \"Z\": Z}\n\n    if return_params:\n        params = {\n            \"beta_x\": self.beta_x,\n            \"beta_y\": self.beta_y,\n            \"noise_scale\": noise_scale,\n            \"covariance_z\": self.covariance_z,\n            \"hypothesis\": \"null\",\n        }\n        result[\"params\"] = params\n\n    return result\n</code></pre>"},{"location":"api/datasets/simulation/#postnonlineargenerator","title":"PostNonlinearGenerator","text":""},{"location":"api/datasets/simulation/#benchbboxtest.datasets.simulation.post_nonlinear.PostNonlinearGenerator","title":"<code>benchbboxtest.datasets.simulation.post_nonlinear.PostNonlinearGenerator</code>","text":"<p>               Bases: <code>DataGenerator</code></p> <p>Post-nonlinear models from Doran et al. (2014).</p> <p>Implements data generation for post-nonlinear models under both null and alternative hypotheses. The models follow the structure:</p> <p>Under null hypothesis (X \u22a5 Y | Z): - Z ~ N(0, I_p) - X = f(Z^T a_x + \u03b5_x), where \u03b5_x ~ N(0, \u03c3\u00b2) - Y = f(Z^T a_y + \u03b5_y), where \u03b5_y ~ N(0, \u03c3\u00b2)</p> <p>Under alternative hypothesis (X \u22a5\u0338 Y | Z): - Z ~ N(0, I_p) - X = f(Z^T a_x + \u03b5_x), where \u03b5_x ~ N(0, \u03c3\u00b2) - Y = f(Z^T a_y + b * X + \u03b5_y), where \u03b5_y ~ N(0, \u03c3\u00b2)</p> <p>where: - p is the dimension of Z - a_x, a_y are coefficient vectors - b controls the strength of dependence between X and Y - \u03c3\u00b2 is the noise variance - f is a nonlinear function (e.g., tanh, sigmoid)</p> Source code in <code>benchbboxtest/datasets/simulation/post_nonlinear.py</code> <pre><code>class PostNonlinearGenerator(DataGenerator):\n    \"\"\"\n    Post-nonlinear models from Doran et al. (2014).\n\n    Implements data generation for post-nonlinear models under both\n    null and alternative hypotheses. The models follow the structure:\n\n    Under null hypothesis (X \u22a5 Y | Z):\n    - Z ~ N(0, I_p)\n    - X = f(Z^T a_x + \u03b5_x), where \u03b5_x ~ N(0, \u03c3\u00b2)\n    - Y = f(Z^T a_y + \u03b5_y), where \u03b5_y ~ N(0, \u03c3\u00b2)\n\n    Under alternative hypothesis (X \u22a5\u0338 Y | Z):\n    - Z ~ N(0, I_p)\n    - X = f(Z^T a_x + \u03b5_x), where \u03b5_x ~ N(0, \u03c3\u00b2)\n    - Y = f(Z^T a_y + b * X + \u03b5_y), where \u03b5_y ~ N(0, \u03c3\u00b2)\n\n    where:\n    - p is the dimension of Z\n    - a_x, a_y are coefficient vectors\n    - b controls the strength of dependence between X and Y\n    - \u03c3\u00b2 is the noise variance\n    - f is a nonlinear function (e.g., tanh, sigmoid)\n    \"\"\"\n\n    # Dictionary of available nonlinear functions\n    NONLINEAR_FUNCTIONS = {\n        \"tanh\": np.tanh,\n        \"sigmoid\": lambda x: 1 / (1 + np.exp(-x)),\n        \"relu\": lambda x: np.maximum(0, x),\n        \"cubic\": lambda x: x**3,\n        \"identity\": lambda x: x,\n    }\n\n    def __init__(\n        self,\n        p: int = 5,\n        a_x: Optional[np.ndarray] = None,\n        a_y: Optional[np.ndarray] = None,\n        nonlinear_func: Union[str, Callable] = \"tanh\",\n        covariance_z: Optional[np.ndarray] = None,\n    ):\n        \"\"\"\n        Initialize the post-nonlinear generator.\n\n        Args:\n            p: Dimension of the conditioning variable Z\n            a_x: Coefficients for X ~ Z relationship (if None, random values are generated)\n            a_y: Coefficients for Y ~ Z relationship (if None, random values are generated)\n            nonlinear_func: Nonlinear function to apply ('tanh', 'sigmoid', 'relu', 'cubic', 'identity') or a callable\n            covariance_z: Covariance matrix for Z (if None, identity matrix is used)\n        \"\"\"\n        self.p = p\n\n        # Set nonlinear function\n        if isinstance(nonlinear_func, str):\n            if nonlinear_func in self.NONLINEAR_FUNCTIONS:\n                self.nonlinear_func = self.NONLINEAR_FUNCTIONS[nonlinear_func]\n            else:\n                raise ValueError(\n                    f\"Unknown nonlinear function: {nonlinear_func}. \"\n                    f\"Available options are: {list(self.NONLINEAR_FUNCTIONS.keys())}\"\n                )\n        elif callable(nonlinear_func):\n            self.nonlinear_func = nonlinear_func\n        else:\n            raise ValueError(\"nonlinear_func must be a string or a callable\")\n\n        # Initialize coefficients if not provided\n        if a_x is None:\n            self.a_x = np.random.normal(0, 1, size=p)\n        else:\n            self.a_x = a_x\n\n        if a_y is None:\n            self.a_y = np.random.normal(0, 1, size=p)\n        else:\n            self.a_y = a_y\n\n        # Initialize covariance matrix for Z\n        if covariance_z is None:\n            self.covariance_z = np.eye(p)  # Identity matrix\n        else:\n            # Ensure the covariance matrix is valid (symmetric and positive semi-definite)\n            if covariance_z.shape != (p, p):\n                raise ValueError(f\"Covariance matrix must have shape ({p}, {p})\")\n            self.covariance_z = covariance_z\n\n    def generate_null(\n        self, n_samples: int, noise_scale: float = 0.5, return_params: bool = False\n    ) -&gt; Dict[str, Union[np.ndarray, Dict]]:\n        \"\"\"\n        Generate data under the null hypothesis (X \u22a5 Y | Z).\n\n        Args:\n            n_samples: Number of samples to generate\n            noise_scale: Scale of the noise (standard deviation)\n            return_params: Whether to return the parameters used for generation\n\n        Returns:\n            Dictionary containing 'X', 'Y', and 'Z' arrays, and optionally 'params'\n        \"\"\"\n        # Generate Z ~ N(0, \u03a3)\n        Z = np.random.multivariate_normal(\n            mean=np.zeros(self.p), cov=self.covariance_z, size=n_samples\n        )\n\n        # Generate X = f(Z * a_x + epsilon_x)\n        epsilon_x = np.random.normal(0, noise_scale, size=n_samples)\n        X_linear = Z @ self.a_x + epsilon_x\n        X = self.nonlinear_func(X_linear)\n\n        # Generate Y = f(Z * a_y + epsilon_y) (independent of X given Z)\n        epsilon_y = np.random.normal(0, noise_scale, size=n_samples)\n        Y_linear = Z @ self.a_y + epsilon_y\n        Y = self.nonlinear_func(Y_linear)\n\n        result = {\"X\": X.reshape(-1, 1), \"Y\": Y.reshape(-1, 1), \"Z\": Z}\n\n        if return_params:\n            params = {\n                \"a_x\": self.a_x,\n                \"a_y\": self.a_y,\n                \"noise_scale\": noise_scale,\n                \"covariance_z\": self.covariance_z,\n                \"nonlinear_func\": self.nonlinear_func.__name__\n                if hasattr(self.nonlinear_func, \"__name__\")\n                else str(self.nonlinear_func),\n                \"hypothesis\": \"null\",\n            }\n            result[\"params\"] = params\n\n        return result\n\n    def generate_alternative(\n        self,\n        n_samples: int,\n        b: float = 0.5,\n        noise_scale: float = 0.5,\n        return_params: bool = False,\n    ) -&gt; Dict[str, Union[np.ndarray, Dict]]:\n        \"\"\"\n        Generate data under the alternative hypothesis (X \u22a5\u0338 Y | Z).\n\n        Args:\n            n_samples: Number of samples to generate\n            b: Strength of dependence between X and Y\n            noise_scale: Scale of the noise (standard deviation)\n            return_params: Whether to return the parameters used for generation\n\n        Returns:\n            Dictionary containing 'X', 'Y', and 'Z' arrays, and optionally 'params'\n        \"\"\"\n        # Generate Z ~ N(0, \u03a3)\n        Z = np.random.multivariate_normal(\n            mean=np.zeros(self.p), cov=self.covariance_z, size=n_samples\n        )\n\n        # Generate X = f(Z * a_x + epsilon_x)\n        epsilon_x = np.random.normal(0, noise_scale, size=n_samples)\n        X_linear = Z @ self.a_x + epsilon_x\n        X = self.nonlinear_func(X_linear)\n\n        # Generate Y = f(Z * a_y + b * X + epsilon_y) (dependent on X given Z)\n        epsilon_y = np.random.normal(0, noise_scale, size=n_samples)\n        Y_linear = Z @ self.a_y + b * X + epsilon_y\n        Y = self.nonlinear_func(Y_linear)\n\n        result = {\"X\": X.reshape(-1, 1), \"Y\": Y.reshape(-1, 1), \"Z\": Z}\n\n        if return_params:\n            params = {\n                \"a_x\": self.a_x,\n                \"a_y\": self.a_y,\n                \"b\": b,\n                \"noise_scale\": noise_scale,\n                \"covariance_z\": self.covariance_z,\n                \"nonlinear_func\": self.nonlinear_func.__name__\n                if hasattr(self.nonlinear_func, \"__name__\")\n                else str(self.nonlinear_func),\n                \"hypothesis\": \"alternative\",\n            }\n            result[\"params\"] = params\n\n        return result\n\n    def generate_dataset(\n        self,\n        n_samples: int,\n        hypothesis: str = \"null\",\n        b: float = 0.5,\n        noise_scale: float = 0.5,\n        return_params: bool = False,\n    ) -&gt; Dict[str, Union[np.ndarray, Dict]]:\n        \"\"\"\n        Generate a dataset under either hypothesis.\n\n        Args:\n            n_samples: Number of samples to generate\n            hypothesis: 'null' or 'alternative'\n            b: Strength of dependence between X and Y (for alternative)\n            noise_scale: Scale of the noise\n            return_params: Whether to return the parameters used for generation\n\n        Returns:\n            Dictionary containing generated data\n        \"\"\"\n        if hypothesis.lower() == \"null\":\n            return self.generate_null(n_samples, noise_scale, return_params)\n        elif hypothesis.lower() == \"alternative\":\n            return self.generate_alternative(n_samples, b, noise_scale, return_params)\n        else:\n            raise ValueError(\"hypothesis must be 'null' or 'alternative'\")\n</code></pre>"},{"location":"api/datasets/simulation/#benchbboxtest.datasets.simulation.post_nonlinear.PostNonlinearGenerator-functions","title":"Functions","text":""},{"location":"api/datasets/simulation/#benchbboxtest.datasets.simulation.post_nonlinear.PostNonlinearGenerator.__init__","title":"<code>__init__(p=5, a_x=None, a_y=None, nonlinear_func='tanh', covariance_z=None)</code>","text":"<p>Initialize the post-nonlinear generator.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>int</code> <p>Dimension of the conditioning variable Z</p> <code>5</code> <code>a_x</code> <code>Optional[ndarray]</code> <p>Coefficients for X ~ Z relationship (if None, random values are generated)</p> <code>None</code> <code>a_y</code> <code>Optional[ndarray]</code> <p>Coefficients for Y ~ Z relationship (if None, random values are generated)</p> <code>None</code> <code>nonlinear_func</code> <code>Union[str, Callable]</code> <p>Nonlinear function to apply ('tanh', 'sigmoid', 'relu', 'cubic', 'identity') or a callable</p> <code>'tanh'</code> <code>covariance_z</code> <code>Optional[ndarray]</code> <p>Covariance matrix for Z (if None, identity matrix is used)</p> <code>None</code> Source code in <code>benchbboxtest/datasets/simulation/post_nonlinear.py</code> <pre><code>def __init__(\n    self,\n    p: int = 5,\n    a_x: Optional[np.ndarray] = None,\n    a_y: Optional[np.ndarray] = None,\n    nonlinear_func: Union[str, Callable] = \"tanh\",\n    covariance_z: Optional[np.ndarray] = None,\n):\n    \"\"\"\n    Initialize the post-nonlinear generator.\n\n    Args:\n        p: Dimension of the conditioning variable Z\n        a_x: Coefficients for X ~ Z relationship (if None, random values are generated)\n        a_y: Coefficients for Y ~ Z relationship (if None, random values are generated)\n        nonlinear_func: Nonlinear function to apply ('tanh', 'sigmoid', 'relu', 'cubic', 'identity') or a callable\n        covariance_z: Covariance matrix for Z (if None, identity matrix is used)\n    \"\"\"\n    self.p = p\n\n    # Set nonlinear function\n    if isinstance(nonlinear_func, str):\n        if nonlinear_func in self.NONLINEAR_FUNCTIONS:\n            self.nonlinear_func = self.NONLINEAR_FUNCTIONS[nonlinear_func]\n        else:\n            raise ValueError(\n                f\"Unknown nonlinear function: {nonlinear_func}. \"\n                f\"Available options are: {list(self.NONLINEAR_FUNCTIONS.keys())}\"\n            )\n    elif callable(nonlinear_func):\n        self.nonlinear_func = nonlinear_func\n    else:\n        raise ValueError(\"nonlinear_func must be a string or a callable\")\n\n    # Initialize coefficients if not provided\n    if a_x is None:\n        self.a_x = np.random.normal(0, 1, size=p)\n    else:\n        self.a_x = a_x\n\n    if a_y is None:\n        self.a_y = np.random.normal(0, 1, size=p)\n    else:\n        self.a_y = a_y\n\n    # Initialize covariance matrix for Z\n    if covariance_z is None:\n        self.covariance_z = np.eye(p)  # Identity matrix\n    else:\n        # Ensure the covariance matrix is valid (symmetric and positive semi-definite)\n        if covariance_z.shape != (p, p):\n            raise ValueError(f\"Covariance matrix must have shape ({p}, {p})\")\n        self.covariance_z = covariance_z\n</code></pre>"},{"location":"api/datasets/simulation/#benchbboxtest.datasets.simulation.post_nonlinear.PostNonlinearGenerator.generate_alternative","title":"<code>generate_alternative(n_samples, b=0.5, noise_scale=0.5, return_params=False)</code>","text":"<p>Generate data under the alternative hypothesis (X \u22a5\u0338 Y | Z).</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Number of samples to generate</p> required <code>b</code> <code>float</code> <p>Strength of dependence between X and Y</p> <code>0.5</code> <code>noise_scale</code> <code>float</code> <p>Scale of the noise (standard deviation)</p> <code>0.5</code> <code>return_params</code> <code>bool</code> <p>Whether to return the parameters used for generation</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Union[ndarray, Dict]]</code> <p>Dictionary containing 'X', 'Y', and 'Z' arrays, and optionally 'params'</p> Source code in <code>benchbboxtest/datasets/simulation/post_nonlinear.py</code> <pre><code>def generate_alternative(\n    self,\n    n_samples: int,\n    b: float = 0.5,\n    noise_scale: float = 0.5,\n    return_params: bool = False,\n) -&gt; Dict[str, Union[np.ndarray, Dict]]:\n    \"\"\"\n    Generate data under the alternative hypothesis (X \u22a5\u0338 Y | Z).\n\n    Args:\n        n_samples: Number of samples to generate\n        b: Strength of dependence between X and Y\n        noise_scale: Scale of the noise (standard deviation)\n        return_params: Whether to return the parameters used for generation\n\n    Returns:\n        Dictionary containing 'X', 'Y', and 'Z' arrays, and optionally 'params'\n    \"\"\"\n    # Generate Z ~ N(0, \u03a3)\n    Z = np.random.multivariate_normal(\n        mean=np.zeros(self.p), cov=self.covariance_z, size=n_samples\n    )\n\n    # Generate X = f(Z * a_x + epsilon_x)\n    epsilon_x = np.random.normal(0, noise_scale, size=n_samples)\n    X_linear = Z @ self.a_x + epsilon_x\n    X = self.nonlinear_func(X_linear)\n\n    # Generate Y = f(Z * a_y + b * X + epsilon_y) (dependent on X given Z)\n    epsilon_y = np.random.normal(0, noise_scale, size=n_samples)\n    Y_linear = Z @ self.a_y + b * X + epsilon_y\n    Y = self.nonlinear_func(Y_linear)\n\n    result = {\"X\": X.reshape(-1, 1), \"Y\": Y.reshape(-1, 1), \"Z\": Z}\n\n    if return_params:\n        params = {\n            \"a_x\": self.a_x,\n            \"a_y\": self.a_y,\n            \"b\": b,\n            \"noise_scale\": noise_scale,\n            \"covariance_z\": self.covariance_z,\n            \"nonlinear_func\": self.nonlinear_func.__name__\n            if hasattr(self.nonlinear_func, \"__name__\")\n            else str(self.nonlinear_func),\n            \"hypothesis\": \"alternative\",\n        }\n        result[\"params\"] = params\n\n    return result\n</code></pre>"},{"location":"api/datasets/simulation/#benchbboxtest.datasets.simulation.post_nonlinear.PostNonlinearGenerator.generate_dataset","title":"<code>generate_dataset(n_samples, hypothesis='null', b=0.5, noise_scale=0.5, return_params=False)</code>","text":"<p>Generate a dataset under either hypothesis.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Number of samples to generate</p> required <code>hypothesis</code> <code>str</code> <p>'null' or 'alternative'</p> <code>'null'</code> <code>b</code> <code>float</code> <p>Strength of dependence between X and Y (for alternative)</p> <code>0.5</code> <code>noise_scale</code> <code>float</code> <p>Scale of the noise</p> <code>0.5</code> <code>return_params</code> <code>bool</code> <p>Whether to return the parameters used for generation</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Union[ndarray, Dict]]</code> <p>Dictionary containing generated data</p> Source code in <code>benchbboxtest/datasets/simulation/post_nonlinear.py</code> <pre><code>def generate_dataset(\n    self,\n    n_samples: int,\n    hypothesis: str = \"null\",\n    b: float = 0.5,\n    noise_scale: float = 0.5,\n    return_params: bool = False,\n) -&gt; Dict[str, Union[np.ndarray, Dict]]:\n    \"\"\"\n    Generate a dataset under either hypothesis.\n\n    Args:\n        n_samples: Number of samples to generate\n        hypothesis: 'null' or 'alternative'\n        b: Strength of dependence between X and Y (for alternative)\n        noise_scale: Scale of the noise\n        return_params: Whether to return the parameters used for generation\n\n    Returns:\n        Dictionary containing generated data\n    \"\"\"\n    if hypothesis.lower() == \"null\":\n        return self.generate_null(n_samples, noise_scale, return_params)\n    elif hypothesis.lower() == \"alternative\":\n        return self.generate_alternative(n_samples, b, noise_scale, return_params)\n    else:\n        raise ValueError(\"hypothesis must be 'null' or 'alternative'\")\n</code></pre>"},{"location":"api/datasets/simulation/#benchbboxtest.datasets.simulation.post_nonlinear.PostNonlinearGenerator.generate_null","title":"<code>generate_null(n_samples, noise_scale=0.5, return_params=False)</code>","text":"<p>Generate data under the null hypothesis (X \u22a5 Y | Z).</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Number of samples to generate</p> required <code>noise_scale</code> <code>float</code> <p>Scale of the noise (standard deviation)</p> <code>0.5</code> <code>return_params</code> <code>bool</code> <p>Whether to return the parameters used for generation</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Union[ndarray, Dict]]</code> <p>Dictionary containing 'X', 'Y', and 'Z' arrays, and optionally 'params'</p> Source code in <code>benchbboxtest/datasets/simulation/post_nonlinear.py</code> <pre><code>def generate_null(\n    self, n_samples: int, noise_scale: float = 0.5, return_params: bool = False\n) -&gt; Dict[str, Union[np.ndarray, Dict]]:\n    \"\"\"\n    Generate data under the null hypothesis (X \u22a5 Y | Z).\n\n    Args:\n        n_samples: Number of samples to generate\n        noise_scale: Scale of the noise (standard deviation)\n        return_params: Whether to return the parameters used for generation\n\n    Returns:\n        Dictionary containing 'X', 'Y', and 'Z' arrays, and optionally 'params'\n    \"\"\"\n    # Generate Z ~ N(0, \u03a3)\n    Z = np.random.multivariate_normal(\n        mean=np.zeros(self.p), cov=self.covariance_z, size=n_samples\n    )\n\n    # Generate X = f(Z * a_x + epsilon_x)\n    epsilon_x = np.random.normal(0, noise_scale, size=n_samples)\n    X_linear = Z @ self.a_x + epsilon_x\n    X = self.nonlinear_func(X_linear)\n\n    # Generate Y = f(Z * a_y + epsilon_y) (independent of X given Z)\n    epsilon_y = np.random.normal(0, noise_scale, size=n_samples)\n    Y_linear = Z @ self.a_y + epsilon_y\n    Y = self.nonlinear_func(Y_linear)\n\n    result = {\"X\": X.reshape(-1, 1), \"Y\": Y.reshape(-1, 1), \"Z\": Z}\n\n    if return_params:\n        params = {\n            \"a_x\": self.a_x,\n            \"a_y\": self.a_y,\n            \"noise_scale\": noise_scale,\n            \"covariance_z\": self.covariance_z,\n            \"nonlinear_func\": self.nonlinear_func.__name__\n            if hasattr(self.nonlinear_func, \"__name__\")\n            else str(self.nonlinear_func),\n            \"hypothesis\": \"null\",\n        }\n        result[\"params\"] = params\n\n    return result\n</code></pre>"},{"location":"api/datasets/text/","title":"Text Dataset Module","text":"<p>This module provides tools for generating text datasets using language models.</p>"},{"location":"api/datasets/text/#llmgenerator","title":"LLMGenerator","text":""},{"location":"api/datasets/text/#benchbboxtest.datasets.text.llm_text.LLMGenerator","title":"<code>benchbboxtest.datasets.text.llm_text.LLMGenerator</code>","text":"<p>Language Model Generator for text generation.</p> Source code in <code>benchbboxtest/datasets/text/llm_text.py</code> <pre><code>class LLMGenerator:\n    \"\"\"\n    Language Model Generator for text generation.\n    \"\"\"\n\n    def __init__(self, model_name: str = \"gpt2\"):\n        \"\"\"\n        Initialize the LLM generator.\n\n        Args:\n            model_name: Name of the pretrained model to use\n        \"\"\"\n        self.model_name = model_name\n        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n\n        # Set device\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model.to(self.device)\n\n    def generate_text(\n        self,\n        prompt: str,\n        max_length: int = 100,\n        temperature: float = 1.0,\n        num_return_sequences: int = 1,\n    ) -&gt; List[str]:\n        \"\"\"\n        Generate text based on a prompt.\n\n        Args:\n            prompt: Input prompt for text generation\n            max_length: Maximum length of the generated text\n            temperature: Sampling temperature (higher = more random)\n            num_return_sequences: Number of sequences to generate\n\n        Returns:\n            List of generated text sequences\n        \"\"\"\n        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n\n        # Generate text\n        outputs = self.model.generate(\n            inputs,\n            max_length=max_length,\n            temperature=temperature,\n            num_return_sequences=num_return_sequences,\n            do_sample=True,\n            pad_token_id=self.tokenizer.eos_token_id,\n        )\n\n        # Decode and return the generated text\n        generated_texts = [\n            self.tokenizer.decode(output, skip_special_tokens=True)\n            for output in outputs\n        ]\n\n        return generated_texts\n</code></pre>"},{"location":"api/datasets/text/#benchbboxtest.datasets.text.llm_text.LLMGenerator-functions","title":"Functions","text":""},{"location":"api/datasets/text/#benchbboxtest.datasets.text.llm_text.LLMGenerator.__init__","title":"<code>__init__(model_name='gpt2')</code>","text":"<p>Initialize the LLM generator.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the pretrained model to use</p> <code>'gpt2'</code> Source code in <code>benchbboxtest/datasets/text/llm_text.py</code> <pre><code>def __init__(self, model_name: str = \"gpt2\"):\n    \"\"\"\n    Initialize the LLM generator.\n\n    Args:\n        model_name: Name of the pretrained model to use\n    \"\"\"\n    self.model_name = model_name\n    self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n    self.model = GPT2LMHeadModel.from_pretrained(model_name)\n\n    # Set device\n    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    self.model.to(self.device)\n</code></pre>"},{"location":"api/datasets/text/#benchbboxtest.datasets.text.llm_text.LLMGenerator.generate_text","title":"<code>generate_text(prompt, max_length=100, temperature=1.0, num_return_sequences=1)</code>","text":"<p>Generate text based on a prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Input prompt for text generation</p> required <code>max_length</code> <code>int</code> <p>Maximum length of the generated text</p> <code>100</code> <code>temperature</code> <code>float</code> <p>Sampling temperature (higher = more random)</p> <code>1.0</code> <code>num_return_sequences</code> <code>int</code> <p>Number of sequences to generate</p> <code>1</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of generated text sequences</p> Source code in <code>benchbboxtest/datasets/text/llm_text.py</code> <pre><code>def generate_text(\n    self,\n    prompt: str,\n    max_length: int = 100,\n    temperature: float = 1.0,\n    num_return_sequences: int = 1,\n) -&gt; List[str]:\n    \"\"\"\n    Generate text based on a prompt.\n\n    Args:\n        prompt: Input prompt for text generation\n        max_length: Maximum length of the generated text\n        temperature: Sampling temperature (higher = more random)\n        num_return_sequences: Number of sequences to generate\n\n    Returns:\n        List of generated text sequences\n    \"\"\"\n    inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n\n    # Generate text\n    outputs = self.model.generate(\n        inputs,\n        max_length=max_length,\n        temperature=temperature,\n        num_return_sequences=num_return_sequences,\n        do_sample=True,\n        pad_token_id=self.tokenizer.eos_token_id,\n    )\n\n    # Decode and return the generated text\n    generated_texts = [\n        self.tokenizer.decode(output, skip_special_tokens=True)\n        for output in outputs\n    ]\n\n    return generated_texts\n</code></pre>"},{"location":"api/datasets/text/#openaigenerator","title":"OpenAIGenerator","text":""},{"location":"api/datasets/text/#benchbboxtest.datasets.text.llm_text.OpenAIGenerator","title":"<code>benchbboxtest.datasets.text.llm_text.OpenAIGenerator</code>","text":"<p>OpenAI API Generator for text generation.</p> <p>This class provides an interface to generate text using OpenAI-compatible APIs. It supports both the official OpenAI API and compatible alternatives.</p> Source code in <code>benchbboxtest/datasets/text/llm_text.py</code> <pre><code>class OpenAIGenerator:\n    \"\"\"\n    OpenAI API Generator for text generation.\n\n    This class provides an interface to generate text using OpenAI-compatible APIs.\n    It supports both the official OpenAI API and compatible alternatives.\n    \"\"\"\n\n    def __init__(\n        self,\n        api_key: str = None,\n        base_url: str = \"https://api.openai.com/v1\",\n        model: str = \"gpt-3.5-turbo\",\n        organization: str = None,\n    ):\n        \"\"\"\n        Initialize the OpenAI generator.\n\n        Args:\n            api_key: OpenAI API key (if None, looks for OPENAI_API_KEY environment variable)\n            base_url: Base URL for the API (can be modified for compatible APIs)\n            model: Model to use for generation\n            organization: OpenAI organization ID (optional)\n        \"\"\"\n        self.api_key = api_key or os.environ.get(\"OPENAI_API_KEY\")\n        if not self.api_key:\n            raise ValueError(\n                \"No API key provided. Set the OPENAI_API_KEY environment variable or pass api_key.\"\n            )\n\n        self.base_url = base_url\n        self.model = model\n        self.organization = organization\n\n        # Determine the endpoint based on the model type\n        if any(model.startswith(prefix) for prefix in [\"gpt-3.5\", \"gpt-4\"]):\n            self.endpoint = f\"{self.base_url}/chat/completions\"\n            self.is_chat_model = True\n        else:\n            self.endpoint = f\"{self.base_url}/completions\"\n            self.is_chat_model = False\n\n    def generate_text(\n        self,\n        prompt: str,\n        max_tokens: int = 100,\n        temperature: float = 1.0,\n        num_return_sequences: int = 1,\n        top_p: float = 1.0,\n        stop: Optional[Union[str, List[str]]] = None,\n        system_message: str = \"You are a helpful assistant.\",\n        **kwargs,\n    ) -&gt; List[str]:\n        \"\"\"\n        Generate text using the OpenAI API or compatible alternatives.\n\n        Args:\n            prompt: Input prompt for text generation\n            max_tokens: Maximum number of tokens to generate\n            temperature: Sampling temperature (higher = more random)\n            num_return_sequences: Number of sequences to generate\n            top_p: Nucleus sampling parameter\n            stop: Sequence(s) at which to stop generation\n            system_message: System message for chat models (ignored for completion models)\n            **kwargs: Additional parameters to pass to the API\n\n        Returns:\n            List of generated text sequences\n        \"\"\"\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {self.api_key}\",\n        }\n\n        if self.organization:\n            headers[\"OpenAI-Organization\"] = self.organization\n\n        # Prepare the request payload based on model type\n        if self.is_chat_model:\n            messages = [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt},\n            ]\n\n            payload = {\n                \"model\": self.model,\n                \"messages\": messages,\n                \"max_tokens\": max_tokens,\n                \"temperature\": temperature,\n                \"n\": num_return_sequences,\n                \"top_p\": top_p,\n            }\n        else:\n            payload = {\n                \"model\": self.model,\n                \"prompt\": prompt,\n                \"max_tokens\": max_tokens,\n                \"temperature\": temperature,\n                \"n\": num_return_sequences,\n                \"top_p\": top_p,\n            }\n\n        # Add stop sequences if provided\n        if stop:\n            payload[\"stop\"] = stop\n\n        # Add any additional parameters\n        for key, value in kwargs.items():\n            payload[key] = value\n\n        try:\n            response = requests.post(self.endpoint, headers=headers, json=payload)\n            response.raise_for_status()\n            response_data = response.json()\n\n            # Extract the generated text based on model type\n            if self.is_chat_model:\n                generated_texts = [\n                    choice[\"message\"][\"content\"] for choice in response_data[\"choices\"]\n                ]\n            else:\n                generated_texts = [\n                    choice[\"text\"] for choice in response_data[\"choices\"]\n                ]\n\n            return generated_texts\n\n        except requests.exceptions.RequestException as e:\n            error_message = f\"API request failed: {e}\"\n\n            if hasattr(e, \"response\") and e.response is not None:\n                try:\n                    error_data = e.response.json()\n                    if \"error\" in error_data:\n                        error_message = f\"API error: {error_data['error']['message']}\"\n                except:\n                    error_message = f\"API error: {e.response.text}\"\n\n            raise RuntimeError(error_message)\n\n    def get_models(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Get the list of available models from the API.\n\n        Returns:\n            List of model objects\n        \"\"\"\n        headers = {\"Authorization\": f\"Bearer {self.api_key}\"}\n\n        if self.organization:\n            headers[\"OpenAI-Organization\"] = self.organization\n\n        try:\n            response = requests.get(f\"{self.base_url}/models\", headers=headers)\n            response.raise_for_status()\n            return response.json()[\"data\"]\n        except requests.exceptions.RequestException as e:\n            raise RuntimeError(f\"Failed to fetch models: {e}\")\n</code></pre>"},{"location":"api/datasets/text/#benchbboxtest.datasets.text.llm_text.OpenAIGenerator-functions","title":"Functions","text":""},{"location":"api/datasets/text/#benchbboxtest.datasets.text.llm_text.OpenAIGenerator.__init__","title":"<code>__init__(api_key=None, base_url='https://api.openai.com/v1', model='gpt-3.5-turbo', organization=None)</code>","text":"<p>Initialize the OpenAI generator.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>OpenAI API key (if None, looks for OPENAI_API_KEY environment variable)</p> <code>None</code> <code>base_url</code> <code>str</code> <p>Base URL for the API (can be modified for compatible APIs)</p> <code>'https://api.openai.com/v1'</code> <code>model</code> <code>str</code> <p>Model to use for generation</p> <code>'gpt-3.5-turbo'</code> <code>organization</code> <code>str</code> <p>OpenAI organization ID (optional)</p> <code>None</code> Source code in <code>benchbboxtest/datasets/text/llm_text.py</code> <pre><code>def __init__(\n    self,\n    api_key: str = None,\n    base_url: str = \"https://api.openai.com/v1\",\n    model: str = \"gpt-3.5-turbo\",\n    organization: str = None,\n):\n    \"\"\"\n    Initialize the OpenAI generator.\n\n    Args:\n        api_key: OpenAI API key (if None, looks for OPENAI_API_KEY environment variable)\n        base_url: Base URL for the API (can be modified for compatible APIs)\n        model: Model to use for generation\n        organization: OpenAI organization ID (optional)\n    \"\"\"\n    self.api_key = api_key or os.environ.get(\"OPENAI_API_KEY\")\n    if not self.api_key:\n        raise ValueError(\n            \"No API key provided. Set the OPENAI_API_KEY environment variable or pass api_key.\"\n        )\n\n    self.base_url = base_url\n    self.model = model\n    self.organization = organization\n\n    # Determine the endpoint based on the model type\n    if any(model.startswith(prefix) for prefix in [\"gpt-3.5\", \"gpt-4\"]):\n        self.endpoint = f\"{self.base_url}/chat/completions\"\n        self.is_chat_model = True\n    else:\n        self.endpoint = f\"{self.base_url}/completions\"\n        self.is_chat_model = False\n</code></pre>"},{"location":"api/datasets/text/#benchbboxtest.datasets.text.llm_text.OpenAIGenerator.generate_text","title":"<code>generate_text(prompt, max_tokens=100, temperature=1.0, num_return_sequences=1, top_p=1.0, stop=None, system_message='You are a helpful assistant.', **kwargs)</code>","text":"<p>Generate text using the OpenAI API or compatible alternatives.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Input prompt for text generation</p> required <code>max_tokens</code> <code>int</code> <p>Maximum number of tokens to generate</p> <code>100</code> <code>temperature</code> <code>float</code> <p>Sampling temperature (higher = more random)</p> <code>1.0</code> <code>num_return_sequences</code> <code>int</code> <p>Number of sequences to generate</p> <code>1</code> <code>top_p</code> <code>float</code> <p>Nucleus sampling parameter</p> <code>1.0</code> <code>stop</code> <code>Optional[Union[str, List[str]]]</code> <p>Sequence(s) at which to stop generation</p> <code>None</code> <code>system_message</code> <code>str</code> <p>System message for chat models (ignored for completion models)</p> <code>'You are a helpful assistant.'</code> <code>**kwargs</code> <p>Additional parameters to pass to the API</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of generated text sequences</p> Source code in <code>benchbboxtest/datasets/text/llm_text.py</code> <pre><code>def generate_text(\n    self,\n    prompt: str,\n    max_tokens: int = 100,\n    temperature: float = 1.0,\n    num_return_sequences: int = 1,\n    top_p: float = 1.0,\n    stop: Optional[Union[str, List[str]]] = None,\n    system_message: str = \"You are a helpful assistant.\",\n    **kwargs,\n) -&gt; List[str]:\n    \"\"\"\n    Generate text using the OpenAI API or compatible alternatives.\n\n    Args:\n        prompt: Input prompt for text generation\n        max_tokens: Maximum number of tokens to generate\n        temperature: Sampling temperature (higher = more random)\n        num_return_sequences: Number of sequences to generate\n        top_p: Nucleus sampling parameter\n        stop: Sequence(s) at which to stop generation\n        system_message: System message for chat models (ignored for completion models)\n        **kwargs: Additional parameters to pass to the API\n\n    Returns:\n        List of generated text sequences\n    \"\"\"\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"Authorization\": f\"Bearer {self.api_key}\",\n    }\n\n    if self.organization:\n        headers[\"OpenAI-Organization\"] = self.organization\n\n    # Prepare the request payload based on model type\n    if self.is_chat_model:\n        messages = [\n            {\"role\": \"system\", \"content\": system_message},\n            {\"role\": \"user\", \"content\": prompt},\n        ]\n\n        payload = {\n            \"model\": self.model,\n            \"messages\": messages,\n            \"max_tokens\": max_tokens,\n            \"temperature\": temperature,\n            \"n\": num_return_sequences,\n            \"top_p\": top_p,\n        }\n    else:\n        payload = {\n            \"model\": self.model,\n            \"prompt\": prompt,\n            \"max_tokens\": max_tokens,\n            \"temperature\": temperature,\n            \"n\": num_return_sequences,\n            \"top_p\": top_p,\n        }\n\n    # Add stop sequences if provided\n    if stop:\n        payload[\"stop\"] = stop\n\n    # Add any additional parameters\n    for key, value in kwargs.items():\n        payload[key] = value\n\n    try:\n        response = requests.post(self.endpoint, headers=headers, json=payload)\n        response.raise_for_status()\n        response_data = response.json()\n\n        # Extract the generated text based on model type\n        if self.is_chat_model:\n            generated_texts = [\n                choice[\"message\"][\"content\"] for choice in response_data[\"choices\"]\n            ]\n        else:\n            generated_texts = [\n                choice[\"text\"] for choice in response_data[\"choices\"]\n            ]\n\n        return generated_texts\n\n    except requests.exceptions.RequestException as e:\n        error_message = f\"API request failed: {e}\"\n\n        if hasattr(e, \"response\") and e.response is not None:\n            try:\n                error_data = e.response.json()\n                if \"error\" in error_data:\n                    error_message = f\"API error: {error_data['error']['message']}\"\n            except:\n                error_message = f\"API error: {e.response.text}\"\n\n        raise RuntimeError(error_message)\n</code></pre>"},{"location":"api/datasets/text/#benchbboxtest.datasets.text.llm_text.OpenAIGenerator.get_models","title":"<code>get_models()</code>","text":"<p>Get the list of available models from the API.</p> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of model objects</p> Source code in <code>benchbboxtest/datasets/text/llm_text.py</code> <pre><code>def get_models(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Get the list of available models from the API.\n\n    Returns:\n        List of model objects\n    \"\"\"\n    headers = {\"Authorization\": f\"Bearer {self.api_key}\"}\n\n    if self.organization:\n        headers[\"OpenAI-Organization\"] = self.organization\n\n    try:\n        response = requests.get(f\"{self.base_url}/models\", headers=headers)\n        response.raise_for_status()\n        return response.json()[\"data\"]\n    except requests.exceptions.RequestException as e:\n        raise RuntimeError(f\"Failed to fetch models: {e}\")\n</code></pre>"},{"location":"api/datasets/text/#arxivcollector","title":"ArXivCollector","text":""},{"location":"api/datasets/text/#benchbboxtest.datasets.text.llm_text.ArXivCollector","title":"<code>benchbboxtest.datasets.text.llm_text.ArXivCollector</code>","text":"<p>Collector for arXiv papers.</p> Source code in <code>benchbboxtest/datasets/text/llm_text.py</code> <pre><code>class ArXivCollector:\n    \"\"\"\n    Collector for arXiv papers.\n    \"\"\"\n\n    def __init__(self, categories: List[str] = None, date_from: str = None):\n        \"\"\"\n        Initialize the arXiv collector.\n\n        Args:\n            categories: List of arXiv categories to search\n            date_from: Date to start searching from (format: YYYY-MM-DD)\n        \"\"\"\n        self.categories = categories if categories else [\"cs.LG\", \"cs.AI\", \"stat.ML\"]\n        self.date_from = date_from\n\n    def search_papers(self, query: str, max_results: int = 100) -&gt; List[Dict]:\n        \"\"\"\n        Search for papers on arXiv.\n\n        Args:\n            query: Search query\n            max_results: Maximum number of results to return\n\n        Returns:\n            List of paper metadata\n        \"\"\"\n        search = arxiv.Search(\n            query=query,\n            max_results=max_results,\n            sort_by=arxiv.SortCriterion.SubmittedDate,\n            sort_order=arxiv.SortOrder.Descending,\n        )\n\n        papers = []\n        for result in search.results():\n            paper = {\n                \"title\": result.title,\n                \"authors\": [author.name for author in result.authors],\n                \"summary\": result.summary,\n                \"published\": result.published,\n                \"updated\": result.updated,\n                \"categories\": result.categories,\n                \"pdf_url\": result.pdf_url,\n            }\n            papers.append(paper)\n\n        return papers\n\n    def download_paper(self, paper_id: str, target_dir: str) -&gt; str:\n        \"\"\"\n        Download a paper from arXiv.\n\n        Args:\n            paper_id: arXiv paper ID\n            target_dir: Directory to save the paper\n\n        Returns:\n            Path to the downloaded paper\n        \"\"\"\n        os.makedirs(target_dir, exist_ok=True)\n\n        # Get paper\n        paper = next(arxiv.Search(id_list=[paper_id]).results())\n\n        # Download PDF\n        target_path = os.path.join(target_dir, f\"{paper_id}.pdf\")\n        paper.download_pdf(target_path)\n\n        return target_path\n</code></pre>"},{"location":"api/datasets/text/#benchbboxtest.datasets.text.llm_text.ArXivCollector-functions","title":"Functions","text":""},{"location":"api/datasets/text/#benchbboxtest.datasets.text.llm_text.ArXivCollector.__init__","title":"<code>__init__(categories=None, date_from=None)</code>","text":"<p>Initialize the arXiv collector.</p> <p>Parameters:</p> Name Type Description Default <code>categories</code> <code>List[str]</code> <p>List of arXiv categories to search</p> <code>None</code> <code>date_from</code> <code>str</code> <p>Date to start searching from (format: YYYY-MM-DD)</p> <code>None</code> Source code in <code>benchbboxtest/datasets/text/llm_text.py</code> <pre><code>def __init__(self, categories: List[str] = None, date_from: str = None):\n    \"\"\"\n    Initialize the arXiv collector.\n\n    Args:\n        categories: List of arXiv categories to search\n        date_from: Date to start searching from (format: YYYY-MM-DD)\n    \"\"\"\n    self.categories = categories if categories else [\"cs.LG\", \"cs.AI\", \"stat.ML\"]\n    self.date_from = date_from\n</code></pre>"},{"location":"api/datasets/text/#benchbboxtest.datasets.text.llm_text.ArXivCollector.download_paper","title":"<code>download_paper(paper_id, target_dir)</code>","text":"<p>Download a paper from arXiv.</p> <p>Parameters:</p> Name Type Description Default <code>paper_id</code> <code>str</code> <p>arXiv paper ID</p> required <code>target_dir</code> <code>str</code> <p>Directory to save the paper</p> required <p>Returns:</p> Type Description <code>str</code> <p>Path to the downloaded paper</p> Source code in <code>benchbboxtest/datasets/text/llm_text.py</code> <pre><code>def download_paper(self, paper_id: str, target_dir: str) -&gt; str:\n    \"\"\"\n    Download a paper from arXiv.\n\n    Args:\n        paper_id: arXiv paper ID\n        target_dir: Directory to save the paper\n\n    Returns:\n        Path to the downloaded paper\n    \"\"\"\n    os.makedirs(target_dir, exist_ok=True)\n\n    # Get paper\n    paper = next(arxiv.Search(id_list=[paper_id]).results())\n\n    # Download PDF\n    target_path = os.path.join(target_dir, f\"{paper_id}.pdf\")\n    paper.download_pdf(target_path)\n\n    return target_path\n</code></pre>"},{"location":"api/datasets/text/#benchbboxtest.datasets.text.llm_text.ArXivCollector.search_papers","title":"<code>search_papers(query, max_results=100)</code>","text":"<p>Search for papers on arXiv.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Search query</p> required <code>max_results</code> <code>int</code> <p>Maximum number of results to return</p> <code>100</code> <p>Returns:</p> Type Description <code>List[Dict]</code> <p>List of paper metadata</p> Source code in <code>benchbboxtest/datasets/text/llm_text.py</code> <pre><code>def search_papers(self, query: str, max_results: int = 100) -&gt; List[Dict]:\n    \"\"\"\n    Search for papers on arXiv.\n\n    Args:\n        query: Search query\n        max_results: Maximum number of results to return\n\n    Returns:\n        List of paper metadata\n    \"\"\"\n    search = arxiv.Search(\n        query=query,\n        max_results=max_results,\n        sort_by=arxiv.SortCriterion.SubmittedDate,\n        sort_order=arxiv.SortOrder.Descending,\n    )\n\n    papers = []\n    for result in search.results():\n        paper = {\n            \"title\": result.title,\n            \"authors\": [author.name for author in result.authors],\n            \"summary\": result.summary,\n            \"published\": result.published,\n            \"updated\": result.updated,\n            \"categories\": result.categories,\n            \"pdf_url\": result.pdf_url,\n        }\n        papers.append(paper)\n\n    return papers\n</code></pre>"},{"location":"api/datasets/text/#textgenerator","title":"TextGenerator","text":""},{"location":"api/datasets/text/#benchbboxtest.datasets.text.llm_text.TextGenerator","title":"<code>benchbboxtest.datasets.text.llm_text.TextGenerator</code>","text":"<p>               Bases: <code>DataGenerator</code></p> <p>Data generator for text-based datasets.</p> <p>Implements data generation for conditional independence testing using text data.</p> Source code in <code>benchbboxtest/datasets/text/llm_text.py</code> <pre><code>class TextGenerator(DataGenerator):\n    \"\"\"\n    Data generator for text-based datasets.\n\n    Implements data generation for conditional independence testing using text data.\n    \"\"\"\n\n    def __init__(\n        self,\n        llm_generator: LLMGenerator = None,\n        context_length: int = 50,\n        prompt_templates: Dict[str, str] = None,\n        vectorizer: Optional[Callable] = None,\n    ):\n        \"\"\"\n        Initialize the text generator.\n\n        Args:\n            llm_generator: Language model generator\n            context_length: Length of the context\n            prompt_templates: Dictionary of prompt templates for different scenarios\n            vectorizer: Function to convert text to numerical vectors (optional)\n        \"\"\"\n        self.llm_generator = llm_generator if llm_generator else LLMGenerator()\n        self.context_length = context_length\n\n        # Default prompt templates\n        self.prompt_templates = (\n            prompt_templates\n            if prompt_templates\n            else {\n                \"null_context\": \"Topic: {context}. Write a paragraph about this topic.\",\n                \"alt_positive\": \"Topic: {context}. Write a positive paragraph about this topic.\",\n                \"alt_negative\": \"Topic: {context}. Write a negative paragraph about this topic.\",\n            }\n        )\n\n        self.vectorizer = vectorizer\n\n    def _generate_contexts(self, n_samples: int) -&gt; List[str]:\n        \"\"\"\n        Generate context topics.\n\n        Args:\n            n_samples: Number of samples to generate\n\n        Returns:\n            List of context strings\n        \"\"\"\n        # Topics from diverse domains\n        topics = [\n            \"Artificial Intelligence\",\n            \"Climate Change\",\n            \"Renewable Energy\",\n            \"Quantum Computing\",\n            \"Blockchain Technology\",\n            \"Space Exploration\",\n            \"Genetic Engineering\",\n            \"Virtual Reality\",\n            \"Robotics\",\n            \"Cybersecurity\",\n            \"Nanotechnology\",\n            \"Internet of Things\",\n            \"Autonomous Vehicles\",\n            \"Machine Learning\",\n            \"Social Media\",\n            \"Biotechnology\",\n            \"3D Printing\",\n            \"Digital Privacy\",\n            \"Cloud Computing\",\n            \"Sustainable Development\",\n        ]\n\n        # Sample topics with replacement if n_samples &gt; len(topics)\n        return np.random.choice(\n            topics, size=n_samples, replace=(n_samples &gt; len(topics))\n        ).tolist()\n\n    def generate_null(\n        self, n_samples: int, temperature: float = 1.0, max_length: int = 150\n    ) -&gt; Dict[str, np.ndarray]:\n        \"\"\"\n        Generate data under the null hypothesis (X \u22a5 Y | Z).\n\n        For the null hypothesis, we generate text using only the context (Z),\n        making X and Y conditionally independent given Z.\n\n        Args:\n            n_samples: Number of samples to generate\n            temperature: Control randomness in generation (higher = more random)\n            max_length: Maximum length of generated text\n\n        Returns:\n            Dictionary containing 'X', 'Y', and 'Z' arrays\n        \"\"\"\n        # Generate contexts (Z)\n        contexts = self._generate_contexts(n_samples)\n\n        # Generate completions (X) based only on context\n        completions = []\n\n        for context in contexts:\n            # Create prompt using the null context template\n            prompt = self.prompt_templates[\"null_context\"].format(context=context)\n\n            # Generate text based only on context\n            generated_text = self.llm_generator.generate_text(\n                prompt, max_length=max_length, temperature=temperature\n            )[0]\n\n            completions.append(generated_text)\n\n        # Assign binary labels (Y) randomly and independently of X given Z\n        # This ensures conditional independence (X \u22a5 Y | Z)\n        labels = np.random.randint(0, 2, size=n_samples)\n\n        # Convert to numpy arrays\n        X = np.array(completions)\n        Y = labels.reshape(-1, 1)\n        Z = np.array(contexts)\n\n        # Apply vectorization if provided\n        if self.vectorizer is not None:\n            X = self.vectorizer(X)\n\n        return {\"X\": X, \"Y\": Y, \"Z\": Z}\n\n    def generate_alternative(\n        self,\n        n_samples: int,\n        dependency_strength: float = 0.8,\n        temperature: float = 0.8,\n        max_length: int = 150,\n    ) -&gt; Dict[str, np.ndarray]:\n        \"\"\"\n        Generate data under the alternative hypothesis (X \u22a5\u0338 Y | Z).\n\n        For the alternative hypothesis, we generate text using both context (Z) and label (Y),\n        making X and Y conditionally dependent given Z.\n\n        Args:\n            n_samples: Number of samples to generate\n            dependency_strength: Controls how strongly Y influences X (0.0-1.0)\n            temperature: Control randomness in generation (higher = more random)\n            max_length: Maximum length of generated text\n\n        Returns:\n            Dictionary containing 'X', 'Y', and 'Z' arrays\n        \"\"\"\n        # Generate contexts (Z)\n        contexts = self._generate_contexts(n_samples)\n\n        # Assign binary labels (Y) randomly\n        labels = np.random.randint(0, 2, size=n_samples)\n\n        # Generate completions (X) based on both context and label\n        completions = []\n\n        for i, context in enumerate(contexts):\n            # Only use label information with probability = dependency_strength\n            # This controls how strong the dependency between X and Y is\n            use_label = np.random.random() &lt; dependency_strength\n\n            if use_label:\n                # Create prompt using either positive or negative template based on the label\n                template_key = \"alt_positive\" if labels[i] == 1 else \"alt_negative\"\n                prompt = self.prompt_templates[template_key].format(context=context)\n            else:\n                # Occasionally use null template to make dependency less deterministic\n                prompt = self.prompt_templates[\"null_context\"].format(context=context)\n\n            # Generate text based on context and possibly label\n            generated_text = self.llm_generator.generate_text(\n                prompt, max_length=max_length, temperature=temperature\n            )[0]\n\n            completions.append(generated_text)\n\n        # Convert to numpy arrays\n        X = np.array(completions)\n        Y = labels.reshape(-1, 1)\n        Z = np.array(contexts)\n\n        # Apply vectorization if provided\n        if self.vectorizer is not None:\n            X = self.vectorizer(X)\n\n        return {\"X\": X, \"Y\": Y, \"Z\": Z}\n</code></pre>"},{"location":"api/datasets/text/#benchbboxtest.datasets.text.llm_text.TextGenerator-functions","title":"Functions","text":""},{"location":"api/datasets/text/#benchbboxtest.datasets.text.llm_text.TextGenerator.__init__","title":"<code>__init__(llm_generator=None, context_length=50, prompt_templates=None, vectorizer=None)</code>","text":"<p>Initialize the text generator.</p> <p>Parameters:</p> Name Type Description Default <code>llm_generator</code> <code>LLMGenerator</code> <p>Language model generator</p> <code>None</code> <code>context_length</code> <code>int</code> <p>Length of the context</p> <code>50</code> <code>prompt_templates</code> <code>Dict[str, str]</code> <p>Dictionary of prompt templates for different scenarios</p> <code>None</code> <code>vectorizer</code> <code>Optional[Callable]</code> <p>Function to convert text to numerical vectors (optional)</p> <code>None</code> Source code in <code>benchbboxtest/datasets/text/llm_text.py</code> <pre><code>def __init__(\n    self,\n    llm_generator: LLMGenerator = None,\n    context_length: int = 50,\n    prompt_templates: Dict[str, str] = None,\n    vectorizer: Optional[Callable] = None,\n):\n    \"\"\"\n    Initialize the text generator.\n\n    Args:\n        llm_generator: Language model generator\n        context_length: Length of the context\n        prompt_templates: Dictionary of prompt templates for different scenarios\n        vectorizer: Function to convert text to numerical vectors (optional)\n    \"\"\"\n    self.llm_generator = llm_generator if llm_generator else LLMGenerator()\n    self.context_length = context_length\n\n    # Default prompt templates\n    self.prompt_templates = (\n        prompt_templates\n        if prompt_templates\n        else {\n            \"null_context\": \"Topic: {context}. Write a paragraph about this topic.\",\n            \"alt_positive\": \"Topic: {context}. Write a positive paragraph about this topic.\",\n            \"alt_negative\": \"Topic: {context}. Write a negative paragraph about this topic.\",\n        }\n    )\n\n    self.vectorizer = vectorizer\n</code></pre>"},{"location":"api/datasets/text/#benchbboxtest.datasets.text.llm_text.TextGenerator.generate_alternative","title":"<code>generate_alternative(n_samples, dependency_strength=0.8, temperature=0.8, max_length=150)</code>","text":"<p>Generate data under the alternative hypothesis (X \u22a5\u0338 Y | Z).</p> <p>For the alternative hypothesis, we generate text using both context (Z) and label (Y), making X and Y conditionally dependent given Z.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Number of samples to generate</p> required <code>dependency_strength</code> <code>float</code> <p>Controls how strongly Y influences X (0.0-1.0)</p> <code>0.8</code> <code>temperature</code> <code>float</code> <p>Control randomness in generation (higher = more random)</p> <code>0.8</code> <code>max_length</code> <code>int</code> <p>Maximum length of generated text</p> <code>150</code> <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>Dictionary containing 'X', 'Y', and 'Z' arrays</p> Source code in <code>benchbboxtest/datasets/text/llm_text.py</code> <pre><code>def generate_alternative(\n    self,\n    n_samples: int,\n    dependency_strength: float = 0.8,\n    temperature: float = 0.8,\n    max_length: int = 150,\n) -&gt; Dict[str, np.ndarray]:\n    \"\"\"\n    Generate data under the alternative hypothesis (X \u22a5\u0338 Y | Z).\n\n    For the alternative hypothesis, we generate text using both context (Z) and label (Y),\n    making X and Y conditionally dependent given Z.\n\n    Args:\n        n_samples: Number of samples to generate\n        dependency_strength: Controls how strongly Y influences X (0.0-1.0)\n        temperature: Control randomness in generation (higher = more random)\n        max_length: Maximum length of generated text\n\n    Returns:\n        Dictionary containing 'X', 'Y', and 'Z' arrays\n    \"\"\"\n    # Generate contexts (Z)\n    contexts = self._generate_contexts(n_samples)\n\n    # Assign binary labels (Y) randomly\n    labels = np.random.randint(0, 2, size=n_samples)\n\n    # Generate completions (X) based on both context and label\n    completions = []\n\n    for i, context in enumerate(contexts):\n        # Only use label information with probability = dependency_strength\n        # This controls how strong the dependency between X and Y is\n        use_label = np.random.random() &lt; dependency_strength\n\n        if use_label:\n            # Create prompt using either positive or negative template based on the label\n            template_key = \"alt_positive\" if labels[i] == 1 else \"alt_negative\"\n            prompt = self.prompt_templates[template_key].format(context=context)\n        else:\n            # Occasionally use null template to make dependency less deterministic\n            prompt = self.prompt_templates[\"null_context\"].format(context=context)\n\n        # Generate text based on context and possibly label\n        generated_text = self.llm_generator.generate_text(\n            prompt, max_length=max_length, temperature=temperature\n        )[0]\n\n        completions.append(generated_text)\n\n    # Convert to numpy arrays\n    X = np.array(completions)\n    Y = labels.reshape(-1, 1)\n    Z = np.array(contexts)\n\n    # Apply vectorization if provided\n    if self.vectorizer is not None:\n        X = self.vectorizer(X)\n\n    return {\"X\": X, \"Y\": Y, \"Z\": Z}\n</code></pre>"},{"location":"api/datasets/text/#benchbboxtest.datasets.text.llm_text.TextGenerator.generate_null","title":"<code>generate_null(n_samples, temperature=1.0, max_length=150)</code>","text":"<p>Generate data under the null hypothesis (X \u22a5 Y | Z).</p> <p>For the null hypothesis, we generate text using only the context (Z), making X and Y conditionally independent given Z.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Number of samples to generate</p> required <code>temperature</code> <code>float</code> <p>Control randomness in generation (higher = more random)</p> <code>1.0</code> <code>max_length</code> <code>int</code> <p>Maximum length of generated text</p> <code>150</code> <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>Dictionary containing 'X', 'Y', and 'Z' arrays</p> Source code in <code>benchbboxtest/datasets/text/llm_text.py</code> <pre><code>def generate_null(\n    self, n_samples: int, temperature: float = 1.0, max_length: int = 150\n) -&gt; Dict[str, np.ndarray]:\n    \"\"\"\n    Generate data under the null hypothesis (X \u22a5 Y | Z).\n\n    For the null hypothesis, we generate text using only the context (Z),\n    making X and Y conditionally independent given Z.\n\n    Args:\n        n_samples: Number of samples to generate\n        temperature: Control randomness in generation (higher = more random)\n        max_length: Maximum length of generated text\n\n    Returns:\n        Dictionary containing 'X', 'Y', and 'Z' arrays\n    \"\"\"\n    # Generate contexts (Z)\n    contexts = self._generate_contexts(n_samples)\n\n    # Generate completions (X) based only on context\n    completions = []\n\n    for context in contexts:\n        # Create prompt using the null context template\n        prompt = self.prompt_templates[\"null_context\"].format(context=context)\n\n        # Generate text based only on context\n        generated_text = self.llm_generator.generate_text(\n            prompt, max_length=max_length, temperature=temperature\n        )[0]\n\n        completions.append(generated_text)\n\n    # Assign binary labels (Y) randomly and independently of X given Z\n    # This ensures conditional independence (X \u22a5 Y | Z)\n    labels = np.random.randint(0, 2, size=n_samples)\n\n    # Convert to numpy arrays\n    X = np.array(completions)\n    Y = labels.reshape(-1, 1)\n    Z = np.array(contexts)\n\n    # Apply vectorization if provided\n    if self.vectorizer is not None:\n        X = self.vectorizer(X)\n\n    return {\"X\": X, \"Y\": Y, \"Z\": Z}\n</code></pre>"},{"location":"api/datasets/text/#ehrtextgenerator","title":"EHRTextGenerator","text":""},{"location":"api/datasets/text/#benchbboxtest.datasets.text.llm_text.EHRTextGenerator","title":"<code>benchbboxtest.datasets.text.llm_text.EHRTextGenerator</code>","text":"<p>               Bases: <code>DataGenerator</code></p> <p>Data generator for Electronic Health Records (EHR) text-based datasets.</p> <p>Implements data generation for conditional independence testing using EHR data, where: - X: Clinical notes or medical reports - Y: Patient outcomes or diagnoses (binary) - Z: Patient demographics and medical history</p> Source code in <code>benchbboxtest/datasets/text/llm_text.py</code> <pre><code>class EHRTextGenerator(DataGenerator):\n    \"\"\"\n    Data generator for Electronic Health Records (EHR) text-based datasets.\n\n    Implements data generation for conditional independence testing using EHR data,\n    where:\n    - X: Clinical notes or medical reports\n    - Y: Patient outcomes or diagnoses (binary)\n    - Z: Patient demographics and medical history\n    \"\"\"\n\n    def __init__(\n        self,\n        llm_generator: LLMGenerator = None,\n        vectorizer: Optional[Callable] = None,\n    ):\n        \"\"\"\n        Initialize the EHR text generator.\n\n        Args:\n            llm_generator: Language model generator\n            vectorizer: Function to convert text to numerical vectors (optional)\n        \"\"\"\n        self.llm_generator = llm_generator if llm_generator else LLMGenerator()\n        self.vectorizer = vectorizer\n\n        # Templates for generating EHR clinical notes\n        self.templates = {\n            # Templates for null hypothesis\n            \"null\": {\n                \"note_template\": \"Patient demographics: {demographics}\\nMedical history: {history}\\n\\nClinical Notes:\",\n            },\n            # Templates for alternative hypothesis (influenced by outcome/diagnosis)\n            \"alt_positive\": {\n                \"note_template\": \"Patient demographics: {demographics}\\nMedical history: {history}\\nDiagnosis: {diagnosis}\\n\\nClinical Notes:\",\n            },\n            \"alt_negative\": {\n                \"note_template\": \"Patient demographics: {demographics}\\nMedical history: {history}\\nNo evidence of: {diagnosis}\\n\\nClinical Notes:\",\n            },\n        }\n\n        # Example demographics, medical histories, and diagnoses\n        self.demographics = [\n            \"45-year-old male\",\n            \"62-year-old female\",\n            \"37-year-old female\",\n            \"53-year-old male\",\n            \"78-year-old female\",\n            \"29-year-old male\",\n            \"41-year-old female\",\n            \"67-year-old male\",\n            \"31-year-old female\",\n            \"59-year-old male\",\n        ]\n\n        self.medical_histories = [\n            \"Hypertension, Type 2 diabetes\",\n            \"Asthma, Seasonal allergies\",\n            \"Hypercholesterolemia, Osteoarthritis\",\n            \"Coronary artery disease, Prior myocardial infarction\",\n            \"COPD, Osteoporosis\",\n            \"No significant past medical history\",\n            \"Hypothyroidism, Depression\",\n            \"Rheumatoid arthritis, Chronic kidney disease\",\n            \"Migraines, Anxiety disorder\",\n            \"Atrial fibrillation, Heart failure\",\n        ]\n\n        self.diagnoses = [\n            \"Pneumonia\",\n            \"Acute myocardial infarction\",\n            \"Urinary tract infection\",\n            \"Community-acquired pneumonia\",\n            \"Congestive heart failure exacerbation\",\n            \"Acute pancreatitis\",\n            \"Diabetic ketoacidosis\",\n            \"Ischemic stroke\",\n            \"Cellulitis\",\n            \"Acute appendicitis\",\n        ]\n\n    def _generate_patient_data(self, n_samples: int) -&gt; Tuple[List[str], List[str]]:\n        \"\"\"\n        Generate patient demographics and medical history.\n\n        Args:\n            n_samples: Number of samples to generate\n\n        Returns:\n            Tuple containing lists of demographics and medical histories\n        \"\"\"\n        # Sample with replacement if n_samples &gt; len(demographics/medical_histories)\n        demographics = np.random.choice(\n            self.demographics,\n            size=n_samples,\n            replace=(n_samples &gt; len(self.demographics)),\n        ).tolist()\n\n        histories = np.random.choice(\n            self.medical_histories,\n            size=n_samples,\n            replace=(n_samples &gt; len(self.medical_histories)),\n        ).tolist()\n\n        return demographics, histories\n\n    def generate_null(\n        self, n_samples: int, temperature: float = 0.7, max_length: int = 200\n    ) -&gt; Dict[str, np.ndarray]:\n        \"\"\"\n        Generate EHR data under the null hypothesis (X \u22a5 Y | Z).\n\n        For the null hypothesis, clinical notes (X) are generated based only on\n        patient demographics and medical history (Z), independent of diagnosis (Y).\n\n        Args:\n            n_samples: Number of samples to generate\n            temperature: Control randomness in generation\n            max_length: Maximum length of generated text\n\n        Returns:\n            Dictionary containing 'X' (clinical notes), 'Y' (diagnoses), and 'Z' (patient info)\n        \"\"\"\n        # Generate patient demographics and medical history (Z)\n        demographics, histories = self._generate_patient_data(n_samples)\n\n        # Combine demographics and histories to form Z\n        Z_data = [f\"{dem}; {hist}\" for dem, hist in zip(demographics, histories)]\n\n        # Generate clinical notes (X) based only on demographics and history\n        clinical_notes = []\n\n        for i in range(n_samples):\n            # Create prompt using the null template\n            prompt = self.templates[\"null\"][\"note_template\"].format(\n                demographics=demographics[i], history=histories[i]\n            )\n\n            # Generate clinical note text based on patient info\n            generated_note = self.llm_generator.generate_text(\n                prompt, max_length=max_length, temperature=temperature\n            )[0]\n\n            clinical_notes.append(generated_note)\n\n        # Assign diagnoses (Y) randomly and independently of X given Z\n        # Note: Binary classification (1: diagnosed, 0: not diagnosed)\n        diagnoses = np.random.randint(0, 2, size=n_samples)\n\n        # Convert to numpy arrays\n        X = np.array(clinical_notes)\n        Y = diagnoses.reshape(-1, 1)\n        Z = np.array(Z_data)\n\n        # Apply vectorization if provided\n        if self.vectorizer is not None:\n            X = self.vectorizer(X)\n\n        return {\"X\": X, \"Y\": Y, \"Z\": Z}\n\n    def generate_alternative(\n        self,\n        n_samples: int,\n        dependency_strength: float = 0.9,\n        temperature: float = 0.7,\n        max_length: int = 200,\n    ) -&gt; Dict[str, np.ndarray]:\n        \"\"\"\n        Generate EHR data under the alternative hypothesis (X \u22a5\u0338 Y | Z).\n\n        For the alternative hypothesis, clinical notes (X) are influenced by both\n        patient info (Z) and diagnosis outcome (Y), making X and Y conditionally dependent.\n\n        Args:\n            n_samples: Number of samples to generate\n            dependency_strength: Controls how strongly diagnosis influences notes (0.0-1.0)\n            temperature: Control randomness in generation\n            max_length: Maximum length of generated text\n\n        Returns:\n            Dictionary containing 'X' (clinical notes), 'Y' (diagnoses), and 'Z' (patient info)\n        \"\"\"\n        # Generate patient demographics and medical history (Z)\n        demographics, histories = self._generate_patient_data(n_samples)\n\n        # Combine demographics and histories to form Z\n        Z_data = [f\"{dem}; {hist}\" for dem, hist in zip(demographics, histories)]\n\n        # Assign diagnoses (Y) randomly\n        # Note: Binary classification (1: diagnosed, 0: not diagnosed)\n        diagnoses = np.random.randint(0, 2, size=n_samples)\n\n        # Sample specific diagnoses from the list\n        specific_diagnoses = np.random.choice(\n            self.diagnoses, size=n_samples, replace=(n_samples &gt; len(self.diagnoses))\n        ).tolist()\n\n        # Generate clinical notes (X) based on patient info and diagnosis\n        clinical_notes = []\n\n        for i in range(n_samples):\n            # Use diagnosis information with probability = dependency_strength\n            # This controls how strong the dependency between notes and diagnosis is\n            use_diagnosis = np.random.random() &lt; dependency_strength\n\n            if use_diagnosis:\n                # Use template that includes diagnosis information\n                template_key = \"alt_positive\" if diagnoses[i] == 1 else \"alt_negative\"\n                prompt = self.templates[template_key][\"note_template\"].format(\n                    demographics=demographics[i],\n                    history=histories[i],\n                    diagnosis=specific_diagnoses[i],\n                )\n            else:\n                # Occasionally use null template to make dependency less deterministic\n                prompt = self.templates[\"null\"][\"note_template\"].format(\n                    demographics=demographics[i], history=histories[i]\n                )\n\n            # Generate clinical note\n            generated_note = self.llm_generator.generate_text(\n                prompt, max_length=max_length, temperature=temperature\n            )[0]\n\n            clinical_notes.append(generated_note)\n\n        # Convert to numpy arrays\n        X = np.array(clinical_notes)\n        Y = diagnoses.reshape(-1, 1)\n        Z = np.array(Z_data)\n\n        # Apply vectorization if provided\n        if self.vectorizer is not None:\n            X = self.vectorizer(X)\n\n        return {\"X\": X, \"Y\": Y, \"Z\": Z}\n\n    def generate_example_dataset(\n        self, n_samples: int = 5, hypothesis: str = \"alternative\"\n    ) -&gt; Dict[str, List]:\n        \"\"\"\n        Generate a small example dataset with human-readable format.\n\n        Args:\n            n_samples: Number of samples to generate\n            hypothesis: 'null' or 'alternative'\n\n        Returns:\n            Dictionary with human-readable examples\n        \"\"\"\n        if hypothesis.lower() == \"null\":\n            data = self.generate_null(n_samples, temperature=0.7)\n        else:\n            data = self.generate_alternative(n_samples, temperature=0.7)\n\n        # Convert numpy arrays to lists for readability\n        examples = {\n            \"clinical_notes\": data[\"X\"].tolist(),\n            \"diagnoses\": data[\"Y\"].reshape(-1).tolist(),\n            \"patient_info\": data[\"Z\"].tolist(),\n            \"hypothesis\": hypothesis,\n        }\n\n        return examples\n</code></pre>"},{"location":"api/datasets/text/#benchbboxtest.datasets.text.llm_text.EHRTextGenerator-functions","title":"Functions","text":""},{"location":"api/datasets/text/#benchbboxtest.datasets.text.llm_text.EHRTextGenerator.__init__","title":"<code>__init__(llm_generator=None, vectorizer=None)</code>","text":"<p>Initialize the EHR text generator.</p> <p>Parameters:</p> Name Type Description Default <code>llm_generator</code> <code>LLMGenerator</code> <p>Language model generator</p> <code>None</code> <code>vectorizer</code> <code>Optional[Callable]</code> <p>Function to convert text to numerical vectors (optional)</p> <code>None</code> Source code in <code>benchbboxtest/datasets/text/llm_text.py</code> <pre><code>def __init__(\n    self,\n    llm_generator: LLMGenerator = None,\n    vectorizer: Optional[Callable] = None,\n):\n    \"\"\"\n    Initialize the EHR text generator.\n\n    Args:\n        llm_generator: Language model generator\n        vectorizer: Function to convert text to numerical vectors (optional)\n    \"\"\"\n    self.llm_generator = llm_generator if llm_generator else LLMGenerator()\n    self.vectorizer = vectorizer\n\n    # Templates for generating EHR clinical notes\n    self.templates = {\n        # Templates for null hypothesis\n        \"null\": {\n            \"note_template\": \"Patient demographics: {demographics}\\nMedical history: {history}\\n\\nClinical Notes:\",\n        },\n        # Templates for alternative hypothesis (influenced by outcome/diagnosis)\n        \"alt_positive\": {\n            \"note_template\": \"Patient demographics: {demographics}\\nMedical history: {history}\\nDiagnosis: {diagnosis}\\n\\nClinical Notes:\",\n        },\n        \"alt_negative\": {\n            \"note_template\": \"Patient demographics: {demographics}\\nMedical history: {history}\\nNo evidence of: {diagnosis}\\n\\nClinical Notes:\",\n        },\n    }\n\n    # Example demographics, medical histories, and diagnoses\n    self.demographics = [\n        \"45-year-old male\",\n        \"62-year-old female\",\n        \"37-year-old female\",\n        \"53-year-old male\",\n        \"78-year-old female\",\n        \"29-year-old male\",\n        \"41-year-old female\",\n        \"67-year-old male\",\n        \"31-year-old female\",\n        \"59-year-old male\",\n    ]\n\n    self.medical_histories = [\n        \"Hypertension, Type 2 diabetes\",\n        \"Asthma, Seasonal allergies\",\n        \"Hypercholesterolemia, Osteoarthritis\",\n        \"Coronary artery disease, Prior myocardial infarction\",\n        \"COPD, Osteoporosis\",\n        \"No significant past medical history\",\n        \"Hypothyroidism, Depression\",\n        \"Rheumatoid arthritis, Chronic kidney disease\",\n        \"Migraines, Anxiety disorder\",\n        \"Atrial fibrillation, Heart failure\",\n    ]\n\n    self.diagnoses = [\n        \"Pneumonia\",\n        \"Acute myocardial infarction\",\n        \"Urinary tract infection\",\n        \"Community-acquired pneumonia\",\n        \"Congestive heart failure exacerbation\",\n        \"Acute pancreatitis\",\n        \"Diabetic ketoacidosis\",\n        \"Ischemic stroke\",\n        \"Cellulitis\",\n        \"Acute appendicitis\",\n    ]\n</code></pre>"},{"location":"api/datasets/text/#benchbboxtest.datasets.text.llm_text.EHRTextGenerator.generate_alternative","title":"<code>generate_alternative(n_samples, dependency_strength=0.9, temperature=0.7, max_length=200)</code>","text":"<p>Generate EHR data under the alternative hypothesis (X \u22a5\u0338 Y | Z).</p> <p>For the alternative hypothesis, clinical notes (X) are influenced by both patient info (Z) and diagnosis outcome (Y), making X and Y conditionally dependent.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Number of samples to generate</p> required <code>dependency_strength</code> <code>float</code> <p>Controls how strongly diagnosis influences notes (0.0-1.0)</p> <code>0.9</code> <code>temperature</code> <code>float</code> <p>Control randomness in generation</p> <code>0.7</code> <code>max_length</code> <code>int</code> <p>Maximum length of generated text</p> <code>200</code> <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>Dictionary containing 'X' (clinical notes), 'Y' (diagnoses), and 'Z' (patient info)</p> Source code in <code>benchbboxtest/datasets/text/llm_text.py</code> <pre><code>def generate_alternative(\n    self,\n    n_samples: int,\n    dependency_strength: float = 0.9,\n    temperature: float = 0.7,\n    max_length: int = 200,\n) -&gt; Dict[str, np.ndarray]:\n    \"\"\"\n    Generate EHR data under the alternative hypothesis (X \u22a5\u0338 Y | Z).\n\n    For the alternative hypothesis, clinical notes (X) are influenced by both\n    patient info (Z) and diagnosis outcome (Y), making X and Y conditionally dependent.\n\n    Args:\n        n_samples: Number of samples to generate\n        dependency_strength: Controls how strongly diagnosis influences notes (0.0-1.0)\n        temperature: Control randomness in generation\n        max_length: Maximum length of generated text\n\n    Returns:\n        Dictionary containing 'X' (clinical notes), 'Y' (diagnoses), and 'Z' (patient info)\n    \"\"\"\n    # Generate patient demographics and medical history (Z)\n    demographics, histories = self._generate_patient_data(n_samples)\n\n    # Combine demographics and histories to form Z\n    Z_data = [f\"{dem}; {hist}\" for dem, hist in zip(demographics, histories)]\n\n    # Assign diagnoses (Y) randomly\n    # Note: Binary classification (1: diagnosed, 0: not diagnosed)\n    diagnoses = np.random.randint(0, 2, size=n_samples)\n\n    # Sample specific diagnoses from the list\n    specific_diagnoses = np.random.choice(\n        self.diagnoses, size=n_samples, replace=(n_samples &gt; len(self.diagnoses))\n    ).tolist()\n\n    # Generate clinical notes (X) based on patient info and diagnosis\n    clinical_notes = []\n\n    for i in range(n_samples):\n        # Use diagnosis information with probability = dependency_strength\n        # This controls how strong the dependency between notes and diagnosis is\n        use_diagnosis = np.random.random() &lt; dependency_strength\n\n        if use_diagnosis:\n            # Use template that includes diagnosis information\n            template_key = \"alt_positive\" if diagnoses[i] == 1 else \"alt_negative\"\n            prompt = self.templates[template_key][\"note_template\"].format(\n                demographics=demographics[i],\n                history=histories[i],\n                diagnosis=specific_diagnoses[i],\n            )\n        else:\n            # Occasionally use null template to make dependency less deterministic\n            prompt = self.templates[\"null\"][\"note_template\"].format(\n                demographics=demographics[i], history=histories[i]\n            )\n\n        # Generate clinical note\n        generated_note = self.llm_generator.generate_text(\n            prompt, max_length=max_length, temperature=temperature\n        )[0]\n\n        clinical_notes.append(generated_note)\n\n    # Convert to numpy arrays\n    X = np.array(clinical_notes)\n    Y = diagnoses.reshape(-1, 1)\n    Z = np.array(Z_data)\n\n    # Apply vectorization if provided\n    if self.vectorizer is not None:\n        X = self.vectorizer(X)\n\n    return {\"X\": X, \"Y\": Y, \"Z\": Z}\n</code></pre>"},{"location":"api/datasets/text/#benchbboxtest.datasets.text.llm_text.EHRTextGenerator.generate_example_dataset","title":"<code>generate_example_dataset(n_samples=5, hypothesis='alternative')</code>","text":"<p>Generate a small example dataset with human-readable format.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Number of samples to generate</p> <code>5</code> <code>hypothesis</code> <code>str</code> <p>'null' or 'alternative'</p> <code>'alternative'</code> <p>Returns:</p> Type Description <code>Dict[str, List]</code> <p>Dictionary with human-readable examples</p> Source code in <code>benchbboxtest/datasets/text/llm_text.py</code> <pre><code>def generate_example_dataset(\n    self, n_samples: int = 5, hypothesis: str = \"alternative\"\n) -&gt; Dict[str, List]:\n    \"\"\"\n    Generate a small example dataset with human-readable format.\n\n    Args:\n        n_samples: Number of samples to generate\n        hypothesis: 'null' or 'alternative'\n\n    Returns:\n        Dictionary with human-readable examples\n    \"\"\"\n    if hypothesis.lower() == \"null\":\n        data = self.generate_null(n_samples, temperature=0.7)\n    else:\n        data = self.generate_alternative(n_samples, temperature=0.7)\n\n    # Convert numpy arrays to lists for readability\n    examples = {\n        \"clinical_notes\": data[\"X\"].tolist(),\n        \"diagnoses\": data[\"Y\"].reshape(-1).tolist(),\n        \"patient_info\": data[\"Z\"].tolist(),\n        \"hypothesis\": hypothesis,\n    }\n\n    return examples\n</code></pre>"},{"location":"api/datasets/text/#benchbboxtest.datasets.text.llm_text.EHRTextGenerator.generate_null","title":"<code>generate_null(n_samples, temperature=0.7, max_length=200)</code>","text":"<p>Generate EHR data under the null hypothesis (X \u22a5 Y | Z).</p> <p>For the null hypothesis, clinical notes (X) are generated based only on patient demographics and medical history (Z), independent of diagnosis (Y).</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Number of samples to generate</p> required <code>temperature</code> <code>float</code> <p>Control randomness in generation</p> <code>0.7</code> <code>max_length</code> <code>int</code> <p>Maximum length of generated text</p> <code>200</code> <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>Dictionary containing 'X' (clinical notes), 'Y' (diagnoses), and 'Z' (patient info)</p> Source code in <code>benchbboxtest/datasets/text/llm_text.py</code> <pre><code>def generate_null(\n    self, n_samples: int, temperature: float = 0.7, max_length: int = 200\n) -&gt; Dict[str, np.ndarray]:\n    \"\"\"\n    Generate EHR data under the null hypothesis (X \u22a5 Y | Z).\n\n    For the null hypothesis, clinical notes (X) are generated based only on\n    patient demographics and medical history (Z), independent of diagnosis (Y).\n\n    Args:\n        n_samples: Number of samples to generate\n        temperature: Control randomness in generation\n        max_length: Maximum length of generated text\n\n    Returns:\n        Dictionary containing 'X' (clinical notes), 'Y' (diagnoses), and 'Z' (patient info)\n    \"\"\"\n    # Generate patient demographics and medical history (Z)\n    demographics, histories = self._generate_patient_data(n_samples)\n\n    # Combine demographics and histories to form Z\n    Z_data = [f\"{dem}; {hist}\" for dem, hist in zip(demographics, histories)]\n\n    # Generate clinical notes (X) based only on demographics and history\n    clinical_notes = []\n\n    for i in range(n_samples):\n        # Create prompt using the null template\n        prompt = self.templates[\"null\"][\"note_template\"].format(\n            demographics=demographics[i], history=histories[i]\n        )\n\n        # Generate clinical note text based on patient info\n        generated_note = self.llm_generator.generate_text(\n            prompt, max_length=max_length, temperature=temperature\n        )[0]\n\n        clinical_notes.append(generated_note)\n\n    # Assign diagnoses (Y) randomly and independently of X given Z\n    # Note: Binary classification (1: diagnosed, 0: not diagnosed)\n    diagnoses = np.random.randint(0, 2, size=n_samples)\n\n    # Convert to numpy arrays\n    X = np.array(clinical_notes)\n    Y = diagnoses.reshape(-1, 1)\n    Z = np.array(Z_data)\n\n    # Apply vectorization if provided\n    if self.vectorizer is not None:\n        X = self.vectorizer(X)\n\n    return {\"X\": X, \"Y\": Y, \"Z\": Z}\n</code></pre>"},{"location":"user-guide/examples/","title":"Examples","text":"<p>This page provides practical examples of using BenchBBoxTest for various tasks.</p>"},{"location":"user-guide/examples/#text-generation-examples","title":"Text Generation Examples","text":""},{"location":"user-guide/examples/#basic-text-generation-with-llmgenerator","title":"Basic Text Generation with LLMGenerator","text":"<pre><code>from benchbboxtest.datasets.text import LLMGenerator\n\n# Initialize the generator\ngenerator = LLMGenerator(model_name=\"gpt2\")\n\n# Generate text with different parameters\ntext1 = generator.generate_text(\n    prompt=\"Artificial intelligence is\",\n    max_length=100,\n    temperature=0.7\n)[0]\n\ntext2 = generator.generate_text(\n    prompt=\"Artificial intelligence is\",\n    max_length=100,\n    temperature=1.5,  # Higher temperature = more random\n    num_return_sequences=3  # Return multiple sequences\n)\n\nprint(f\"Generated text (temperature=0.7):\\n{text1}\\n\")\nprint(f\"Generated texts (temperature=1.5):\")\nfor i, text in enumerate(text2):\n    print(f\"Sequence {i+1}:\\n{text}\\n\")\n</code></pre>"},{"location":"user-guide/examples/#using-openai-api","title":"Using OpenAI API","text":"<pre><code>import os\nfrom benchbboxtest.datasets.text import OpenAIGenerator\n\n# Set your API key as an environment variable\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n\n# Initialize the generator\ngenerator = OpenAIGenerator(model=\"gpt-3.5-turbo\")\n\n# Generate text using the chat model\nresponse = generator.generate_text(\n    prompt=\"Explain the concept of black-box testing in simple terms.\",\n    max_tokens=150,\n    temperature=0.7,\n    system_message=\"You are a helpful assistant that explains complex topics simply.\"\n)[0]\n\nprint(response)\n</code></pre>"},{"location":"user-guide/examples/#conditional-independence-testing","title":"Conditional Independence Testing","text":""},{"location":"user-guide/examples/#generating-null-and-alternative-datasets","title":"Generating Null and Alternative Datasets","text":"<pre><code>from benchbboxtest.datasets.text import LLMGenerator, TextGenerator\n\n# Initialize generators\nllm = LLMGenerator(model_name=\"gpt2\")\ntext_gen = TextGenerator(llm_generator=llm)\n\n# Generate data under the null hypothesis (X \u22a5 Y | Z)\nnull_data = text_gen.generate_null(\n    n_samples=50,\n    temperature=0.8,\n    max_length=150\n)\n\n# Generate data under the alternative hypothesis (X \u22a5\u0338 Y | Z)\n# with strong dependency between X and Y given Z\nalt_data_strong = text_gen.generate_alternative(\n    n_samples=50,\n    dependency_strength=0.9,  # Strong dependency\n    temperature=0.8,\n    max_length=150\n)\n\n# Generate data with weaker dependency\nalt_data_weak = text_gen.generate_alternative(\n    n_samples=50,\n    dependency_strength=0.3,  # Weak dependency\n    temperature=0.8,\n    max_length=150\n)\n\n# Check the shapes of the generated data\nprint(f\"Null data shapes:\")\nprint(f\"X: {null_data['X'].shape}, Y: {null_data['Y'].shape}, Z: {null_data['Z'].shape}\")\n</code></pre>"},{"location":"user-guide/examples/#using-ehr-text-generator","title":"Using EHR Text Generator","text":"<p>```python from benchbboxtest.datasets.text import LLMGenerator, EHRTextGenerator</p>"},{"location":"user-guide/examples/#initialize-generators","title":"Initialize generators","text":"<p>llm = LLMGenerator(model_name=\"gpt2\") ehr_gen = EHRTextGenerator(llm_generator=llm)</p>"},{"location":"user-guide/examples/#generate-a-small-example-dataset-under-the-alternative-hypothesis","title":"Generate a small example dataset under the alternative hypothesis","text":"<p>examples = ehr_gen.generate_example_dataset(     n_samples=3,     hypothesis=\"alternative\" )</p>"},{"location":"user-guide/examples/#print-the-results","title":"Print the results","text":"<p>for i in range(len(examples[\"clinical_notes\"])):     print(f\"EXAMPLE {i+1}\")     print(f\"Patient Info: {examples['patient_info'][i]}\")     print(f\"Diagnosis: {'Positive' if examples['diagnoses'][i] == 1 else 'Negative'}\")     print(f\"Clinical Note: {examples['clinical_notes'][i][:200]}...\\n\") </p>"},{"location":"user-guide/installation/","title":"Installation","text":"<p>This page provides detailed instructions for installing BenchBBoxTest in different environments.</p>"},{"location":"user-guide/installation/#requirements","title":"Requirements","text":"<p>BenchBBoxTest requires:</p> <ul> <li>Python 3.8 or higher</li> <li>PyTorch (for language model functionality)</li> <li>NumPy</li> <li>Transformers (for pre-trained language models)</li> </ul>"},{"location":"user-guide/installation/#standard-installation","title":"Standard Installation","text":"<p>The easiest way to install BenchBBoxTest is using pip:</p> <pre><code>pip install benchbboxtest\n</code></pre> <p>This will install the package and all its required dependencies.</p>"},{"location":"user-guide/installation/#installation-from-source","title":"Installation from Source","text":"<p>For the latest features or to contribute to development, you can install from source:</p> <pre><code>git clone https://github.com/jiantingfeng/BenchBBoxTest.git\ncd BenchBBoxTest\npip install -e .\n</code></pre>"},{"location":"user-guide/installation/#gpu-support","title":"GPU Support","text":"<p>Some features of BenchBBoxTest, particularly the language model functionality, can benefit significantly from GPU acceleration. To enable GPU support:</p> <ol> <li>Make sure you have the appropriate CUDA drivers installed</li> <li>Install PyTorch with CUDA support following the instructions at pytorch.org</li> </ol>"},{"location":"user-guide/installation/#optional-dependencies","title":"Optional Dependencies","text":""},{"location":"user-guide/installation/#for-openai-integration","title":"For OpenAI Integration","text":"<p>To use the OpenAI API functionality:</p> <pre><code>pip install openai\n</code></pre>"},{"location":"user-guide/installation/#for-arxiv-integration","title":"For ArXiv Integration","text":"<p>To use the ArXiv paper collection functionality:</p> <pre><code>pip install arxiv\n</code></pre>"},{"location":"user-guide/overview/","title":"Overview","text":"<p>BenchBBoxTest is a Python package designed to provide tools for benchmarking black-box testing methods, with a focus on conditional independence testing in various data domains.</p>"},{"location":"user-guide/overview/#package-structure","title":"Package Structure","text":"<p>The package is organized into several main components:</p>"},{"location":"user-guide/overview/#core","title":"Core","text":"<p>The core module provides the fundamental abstractions and base classes used throughout the package, including data generators and test interfaces.</p>"},{"location":"user-guide/overview/#datasets","title":"Datasets","text":"<p>The datasets module contains tools for generating synthetic data in different domains:</p> <ul> <li>Text: Language model-based text generation tools</li> <li>Image: Image generation utilities</li> <li>Simulation: Data simulation from various distributions</li> </ul>"},{"location":"user-guide/overview/#evaluation","title":"Evaluation","text":"<p>The evaluation module provides metrics and evaluation tools to assess the performance of testing methods.</p>"},{"location":"user-guide/overview/#utils","title":"Utils","text":"<p>The utils module includes helper functions and utilities used across the package.</p>"},{"location":"user-guide/overview/#key-concepts","title":"Key Concepts","text":""},{"location":"user-guide/overview/#conditional-independence-testing","title":"Conditional Independence Testing","text":"<p>A core focus of BenchBBoxTest is conditional independence testing, which examines whether two variables X and Y are independent given a third variable Z.</p> <ul> <li>Null Hypothesis (H\u2080): X and Y are conditionally independent given Z (X \u22a5 Y | Z)</li> <li>Alternative Hypothesis (H\u2081): X and Y are conditionally dependent given Z (X \u22a5\u0338 Y | Z)</li> </ul>"},{"location":"user-guide/overview/#data-generators","title":"Data Generators","text":"<p>BenchBBoxTest provides data generators that can create synthetic datasets under both null and alternative hypotheses for benchmarking testing methods. These generators allow you to control:</p> <ul> <li>Sample size</li> <li>Dependency strength</li> <li>Noise levels</li> <li>Data domain-specific parameters </li> </ul>"}]}